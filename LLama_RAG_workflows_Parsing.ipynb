{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b2060be6-ec2f-4a5a-b0f4-61dc93139732",
      "metadata": {
        "id": "b2060be6-ec2f-4a5a-b0f4-61dc93139732"
      },
      "source": [
        "# Lesson 3: Adding RAG"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74fd47f0-692f-4fe4-8f0d-7b8fbe0a894c",
      "metadata": {
        "id": "74fd47f0-692f-4fe4-8f0d-7b8fbe0a894c"
      },
      "source": [
        "**Lesson objective**: Add a document database to a workflow\n",
        "\n",
        "In this lab, you’ll parse a resume and load it into a vector store, and use the agent to run basic queries against the documents. You’ll use LlamaParse to parse the documents."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aaaee4d2",
      "metadata": {
        "id": "aaaee4d2"
      },
      "source": [
        "<div style=\"background-color:#fff1d7; padding:15px;\"> <b> Note</b>: Make sure to run the notebook cell by cell. Please try to avoid running all cells at once.</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7vQzyNaS8mOJ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7vQzyNaS8mOJ",
        "outputId": "01cd5944-e2fa-466f-9d8a-036dbcbd67f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: llama-index-core in /usr/local/lib/python3.11/dist-packages (0.12.50)\n",
            "Requirement already satisfied: aiohttp<4,>=3.8.6 in /usr/local/lib/python3.11/dist-packages (from llama-index-core) (3.11.15)\n",
            "Requirement already satisfied: aiosqlite in /usr/local/lib/python3.11/dist-packages (from llama-index-core) (0.21.0)\n",
            "Requirement already satisfied: banks<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core) (2.2.0)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from llama-index-core) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core) (1.2.18)\n",
            "Requirement already satisfied: dirtyjson<2,>=1.0.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core) (1.0.8)\n",
            "Requirement already satisfied: filetype<2,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core) (1.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core) (2025.3.2)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from llama-index-core) (0.28.1)\n",
            "Requirement already satisfied: llama-index-workflows<2,>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core) (1.1.0)\n",
            "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core) (3.5)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from llama-index-core) (2.0.2)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core) (11.2.1)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from llama-index-core) (4.3.8)\n",
            "Requirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core) (2.11.7)\n",
            "Requirement already satisfied: pyyaml>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=80.9.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core) (80.9.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core) (2.0.41)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core) (8.5.0)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core) (0.9.0)\n",
            "Requirement already satisfied: tqdm<5,>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core) (4.14.1)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core) (1.17.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core) (1.20.1)\n",
            "Requirement already satisfied: griffe in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.0.0->llama-index-core) (1.7.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.0.0->llama-index-core) (3.1.6)\n",
            "Requirement already satisfied: llama-index-instrumentation>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-workflows<2,>=1.0.1->llama-index-core) (0.3.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core) (2024.11.6)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core) (2025.7.14)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core) (3.2.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect>=0.8.0->llama-index-core) (1.1.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->llama-index-core) (3.26.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->llama-index-core) (0.16.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.11/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core) (25.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx->llama-index-core) (1.3.1)\n",
            "Requirement already satisfied: colorama>=0.4 in /usr/local/lib/python3.11/dist-packages (from griffe->banks<3,>=2.0.0->llama-index-core) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->banks<3,>=2.0.0->llama-index-core) (3.0.2)\n",
            "Requirement already satisfied: llama-index-utils-workflow in /usr/local/lib/python3.11/dist-packages (0.3.5)\n",
            "Requirement already satisfied: llama-index-core<0.13,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-utils-workflow) (0.12.50)\n",
            "Requirement already satisfied: pyvis<0.4,>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from llama-index-utils-workflow) (0.3.2)\n",
            "Requirement already satisfied: aiohttp<4,>=3.8.6 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama-index-utils-workflow) (3.11.15)\n",
            "Requirement already satisfied: aiosqlite in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama-index-utils-workflow) (0.21.0)\n",
            "Requirement already satisfied: banks<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama-index-utils-workflow) (2.2.0)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama-index-utils-workflow) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama-index-utils-workflow) (1.2.18)\n",
            "Requirement already satisfied: dirtyjson<2,>=1.0.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama-index-utils-workflow) (1.0.8)\n",
            "Requirement already satisfied: filetype<2,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama-index-utils-workflow) (1.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama-index-utils-workflow) (2025.3.2)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama-index-utils-workflow) (0.28.1)\n",
            "Requirement already satisfied: llama-index-workflows<2,>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama-index-utils-workflow) (1.1.0)\n",
            "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama-index-utils-workflow) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama-index-utils-workflow) (3.5)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama-index-utils-workflow) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama-index-utils-workflow) (2.0.2)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama-index-utils-workflow) (11.2.1)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama-index-utils-workflow) (4.3.8)\n",
            "Requirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama-index-utils-workflow) (2.11.7)\n",
            "Requirement already satisfied: pyyaml>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama-index-utils-workflow) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama-index-utils-workflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=80.9.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama-index-utils-workflow) (80.9.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.13,>=0.12.0->llama-index-utils-workflow) (2.0.41)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama-index-utils-workflow) (8.5.0)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama-index-utils-workflow) (0.9.0)\n",
            "Requirement already satisfied: tqdm<5,>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama-index-utils-workflow) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama-index-utils-workflow) (4.14.1)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama-index-utils-workflow) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama-index-utils-workflow) (1.17.2)\n",
            "Requirement already satisfied: ipython>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from pyvis<0.4,>=0.3.2->llama-index-utils-workflow) (7.34.0)\n",
            "Requirement already satisfied: jinja2>=2.9.6 in /usr/local/lib/python3.11/dist-packages (from pyvis<0.4,>=0.3.2->llama-index-utils-workflow) (3.1.6)\n",
            "Requirement already satisfied: jsonpickle>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from pyvis<0.4,>=0.3.2->llama-index-utils-workflow) (4.1.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.0->llama-index-utils-workflow) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.0->llama-index-utils-workflow) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.0->llama-index-utils-workflow) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.0->llama-index-utils-workflow) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.0->llama-index-utils-workflow) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.0->llama-index-utils-workflow) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.0->llama-index-utils-workflow) (1.20.1)\n",
            "Requirement already satisfied: griffe in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.0->llama-index-utils-workflow) (1.7.3)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis<0.4,>=0.3.2->llama-index-utils-workflow) (0.19.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis<0.4,>=0.3.2->llama-index-utils-workflow) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis<0.4,>=0.3.2->llama-index-utils-workflow) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis<0.4,>=0.3.2->llama-index-utils-workflow) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis<0.4,>=0.3.2->llama-index-utils-workflow) (3.0.51)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis<0.4,>=0.3.2->llama-index-utils-workflow) (2.19.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis<0.4,>=0.3.2->llama-index-utils-workflow) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis<0.4,>=0.3.2->llama-index-utils-workflow) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis<0.4,>=0.3.2->llama-index-utils-workflow) (4.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.9.6->pyvis<0.4,>=0.3.2->llama-index-utils-workflow) (3.0.2)\n",
            "Requirement already satisfied: llama-index-instrumentation>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-workflows<2,>=1.0.1->llama-index-core<0.13,>=0.12.0->llama-index-utils-workflow) (0.3.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13,>=0.12.0->llama-index-utils-workflow) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13,>=0.12.0->llama-index-utils-workflow) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13,>=0.12.0->llama-index-utils-workflow) (2024.11.6)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13,>=0.12.0->llama-index-utils-workflow) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13,>=0.12.0->llama-index-utils-workflow) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13,>=0.12.0->llama-index-utils-workflow) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13,>=0.12.0->llama-index-utils-workflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13,>=0.12.0->llama-index-utils-workflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13,>=0.12.0->llama-index-utils-workflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13,>=0.12.0->llama-index-utils-workflow) (2025.7.14)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.13,>=0.12.0->llama-index-utils-workflow) (3.2.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.13,>=0.12.0->llama-index-utils-workflow) (1.1.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->llama-index-core<0.13,>=0.12.0->llama-index-utils-workflow) (3.26.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13,>=0.12.0->llama-index-utils-workflow) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13,>=0.12.0->llama-index-utils-workflow) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.13,>=0.12.0->llama-index-utils-workflow) (0.16.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=5.3.0->pyvis<0.4,>=0.3.2->llama-index-utils-workflow) (0.8.4)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.11/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13,>=0.12.0->llama-index-utils-workflow) (25.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=5.3.0->pyvis<0.4,>=0.3.2->llama-index-utils-workflow) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.3.0->pyvis<0.4,>=0.3.2->llama-index-utils-workflow) (0.2.13)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx->llama-index-core<0.13,>=0.12.0->llama-index-utils-workflow) (1.3.1)\n",
            "Requirement already satisfied: colorama>=0.4 in /usr/local/lib/python3.11/dist-packages (from griffe->banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.0->llama-index-utils-workflow) (0.4.6)\n",
            "Requirement already satisfied: llama-index-llms-openai in /usr/local/lib/python3.11/dist-packages (0.4.7)\n",
            "Requirement already satisfied: llama-index-core<0.13,>=0.12.41 in /usr/local/lib/python3.11/dist-packages (from llama-index-llms-openai) (0.12.50)\n",
            "Requirement already satisfied: openai<2,>=1.81.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-llms-openai) (1.97.0)\n",
            "Requirement already satisfied: aiohttp<4,>=3.8.6 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.41->llama-index-llms-openai) (3.11.15)\n",
            "Requirement already satisfied: aiosqlite in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.41->llama-index-llms-openai) (0.21.0)\n",
            "Requirement already satisfied: banks<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.41->llama-index-llms-openai) (2.2.0)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.41->llama-index-llms-openai) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.41->llama-index-llms-openai) (1.2.18)\n",
            "Requirement already satisfied: dirtyjson<2,>=1.0.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.41->llama-index-llms-openai) (1.0.8)\n",
            "Requirement already satisfied: filetype<2,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.41->llama-index-llms-openai) (1.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.41->llama-index-llms-openai) (2025.3.2)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.41->llama-index-llms-openai) (0.28.1)\n",
            "Requirement already satisfied: llama-index-workflows<2,>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.41->llama-index-llms-openai) (1.1.0)\n",
            "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.41->llama-index-llms-openai) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.41->llama-index-llms-openai) (3.5)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.41->llama-index-llms-openai) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.41->llama-index-llms-openai) (2.0.2)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.41->llama-index-llms-openai) (11.2.1)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.41->llama-index-llms-openai) (4.3.8)\n",
            "Requirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.41->llama-index-llms-openai) (2.11.7)\n",
            "Requirement already satisfied: pyyaml>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.41->llama-index-llms-openai) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.41->llama-index-llms-openai) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=80.9.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.41->llama-index-llms-openai) (80.9.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.13,>=0.12.41->llama-index-llms-openai) (2.0.41)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.41->llama-index-llms-openai) (8.5.0)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.41->llama-index-llms-openai) (0.9.0)\n",
            "Requirement already satisfied: tqdm<5,>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.41->llama-index-llms-openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.41->llama-index-llms-openai) (4.14.1)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.41->llama-index-llms-openai) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.41->llama-index-llms-openai) (1.17.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2,>=1.81.0->llama-index-llms-openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2,>=1.81.0->llama-index-llms-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2,>=1.81.0->llama-index-llms-openai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2,>=1.81.0->llama-index-llms-openai) (1.3.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.41->llama-index-llms-openai) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.41->llama-index-llms-openai) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.41->llama-index-llms-openai) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.41->llama-index-llms-openai) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.41->llama-index-llms-openai) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.41->llama-index-llms-openai) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.41->llama-index-llms-openai) (1.20.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai<2,>=1.81.0->llama-index-llms-openai) (3.10)\n",
            "Requirement already satisfied: griffe in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.41->llama-index-llms-openai) (1.7.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.41->llama-index-llms-openai) (3.1.6)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13,>=0.12.41->llama-index-llms-openai) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13,>=0.12.41->llama-index-llms-openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.13,>=0.12.41->llama-index-llms-openai) (0.16.0)\n",
            "Requirement already satisfied: llama-index-instrumentation>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-workflows<2,>=1.0.1->llama-index-core<0.13,>=0.12.41->llama-index-llms-openai) (0.3.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13,>=0.12.41->llama-index-llms-openai) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13,>=0.12.41->llama-index-llms-openai) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13,>=0.12.41->llama-index-llms-openai) (2024.11.6)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13,>=0.12.41->llama-index-llms-openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13,>=0.12.41->llama-index-llms-openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13,>=0.12.41->llama-index-llms-openai) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13,>=0.12.41->llama-index-llms-openai) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13,>=0.12.41->llama-index-llms-openai) (2.4.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.13,>=0.12.41->llama-index-llms-openai) (3.2.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.13,>=0.12.41->llama-index-llms-openai) (1.1.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->llama-index-core<0.13,>=0.12.41->llama-index-llms-openai) (3.26.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.11/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13,>=0.12.41->llama-index-llms-openai) (25.0)\n",
            "Requirement already satisfied: colorama>=0.4 in /usr/local/lib/python3.11/dist-packages (from griffe->banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.41->llama-index-llms-openai) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.41->llama-index-llms-openai) (3.0.2)\n",
            "Collecting llama-parse\n",
            "  Downloading llama_parse-0.6.50-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting llama-cloud-services>=0.6.49 (from llama-parse)\n",
            "  Downloading llama_cloud_services-0.6.51-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: click<9.0.0,>=8.1.7 in /usr/local/lib/python3.11/dist-packages (from llama-cloud-services>=0.6.49->llama-parse) (8.2.1)\n",
            "Collecting llama-cloud==0.1.34 (from llama-cloud-services>=0.6.49->llama-parse)\n",
            "  Downloading llama_cloud-0.1.34-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: llama-index-core>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from llama-cloud-services>=0.6.49->llama-parse) (0.12.50)\n",
            "Requirement already satisfied: platformdirs<5.0.0,>=4.3.7 in /usr/local/lib/python3.11/dist-packages (from llama-cloud-services>=0.6.49->llama-parse) (4.3.8)\n",
            "Requirement already satisfied: pydantic!=2.10,>=2.8 in /usr/local/lib/python3.11/dist-packages (from llama-cloud-services>=0.6.49->llama-parse) (2.11.7)\n",
            "Collecting python-dotenv<2.0.0,>=1.0.1 (from llama-cloud-services>=0.6.49->llama-parse)\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: tenacity<10.0,>=8.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-cloud-services>=0.6.49->llama-parse) (8.5.0)\n",
            "Requirement already satisfied: certifi>=2024.7.4 in /usr/local/lib/python3.11/dist-packages (from llama-cloud==0.1.34->llama-cloud-services>=0.6.49->llama-parse) (2025.7.14)\n",
            "Requirement already satisfied: httpx>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from llama-cloud==0.1.34->llama-cloud-services>=0.6.49->llama-parse) (0.28.1)\n",
            "Requirement already satisfied: aiohttp<4,>=3.8.6 in /usr/local/lib/python3.11/dist-packages (from llama-index-core>=0.12.0->llama-cloud-services>=0.6.49->llama-parse) (3.11.15)\n",
            "Requirement already satisfied: aiosqlite in /usr/local/lib/python3.11/dist-packages (from llama-index-core>=0.12.0->llama-cloud-services>=0.6.49->llama-parse) (0.21.0)\n",
            "Requirement already satisfied: banks<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core>=0.12.0->llama-cloud-services>=0.6.49->llama-parse) (2.2.0)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from llama-index-core>=0.12.0->llama-cloud-services>=0.6.49->llama-parse) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core>=0.12.0->llama-cloud-services>=0.6.49->llama-parse) (1.2.18)\n",
            "Requirement already satisfied: dirtyjson<2,>=1.0.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core>=0.12.0->llama-cloud-services>=0.6.49->llama-parse) (1.0.8)\n",
            "Requirement already satisfied: filetype<2,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core>=0.12.0->llama-cloud-services>=0.6.49->llama-parse) (1.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core>=0.12.0->llama-cloud-services>=0.6.49->llama-parse) (2025.3.2)\n",
            "Requirement already satisfied: llama-index-workflows<2,>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core>=0.12.0->llama-cloud-services>=0.6.49->llama-parse) (1.1.0)\n",
            "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core>=0.12.0->llama-cloud-services>=0.6.49->llama-parse) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core>=0.12.0->llama-cloud-services>=0.6.49->llama-parse) (3.5)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core>=0.12.0->llama-cloud-services>=0.6.49->llama-parse) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from llama-index-core>=0.12.0->llama-cloud-services>=0.6.49->llama-parse) (2.0.2)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core>=0.12.0->llama-cloud-services>=0.6.49->llama-parse) (11.2.1)\n",
            "Requirement already satisfied: pyyaml>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core>=0.12.0->llama-cloud-services>=0.6.49->llama-parse) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core>=0.12.0->llama-cloud-services>=0.6.49->llama-parse) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=80.9.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core>=0.12.0->llama-cloud-services>=0.6.49->llama-parse) (80.9.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core>=0.12.0->llama-cloud-services>=0.6.49->llama-parse) (2.0.41)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core>=0.12.0->llama-cloud-services>=0.6.49->llama-parse) (0.9.0)\n",
            "Requirement already satisfied: tqdm<5,>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core>=0.12.0->llama-cloud-services>=0.6.49->llama-parse) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core>=0.12.0->llama-cloud-services>=0.6.49->llama-parse) (4.14.1)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core>=0.12.0->llama-cloud-services>=0.6.49->llama-parse) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core>=0.12.0->llama-cloud-services>=0.6.49->llama-parse) (1.17.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=2.10,>=2.8->llama-cloud-services>=0.6.49->llama-parse) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=2.10,>=2.8->llama-cloud-services>=0.6.49->llama-parse) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=2.10,>=2.8->llama-cloud-services>=0.6.49->llama-parse) (0.4.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core>=0.12.0->llama-cloud-services>=0.6.49->llama-parse) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core>=0.12.0->llama-cloud-services>=0.6.49->llama-parse) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core>=0.12.0->llama-cloud-services>=0.6.49->llama-parse) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core>=0.12.0->llama-cloud-services>=0.6.49->llama-parse) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core>=0.12.0->llama-cloud-services>=0.6.49->llama-parse) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core>=0.12.0->llama-cloud-services>=0.6.49->llama-parse) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core>=0.12.0->llama-cloud-services>=0.6.49->llama-parse) (1.20.1)\n",
            "Requirement already satisfied: griffe in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.0.0->llama-index-core>=0.12.0->llama-cloud-services>=0.6.49->llama-parse) (1.7.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.0.0->llama-index-core>=0.12.0->llama-cloud-services>=0.6.49->llama-parse) (3.1.6)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.20.0->llama-cloud==0.1.34->llama-cloud-services>=0.6.49->llama-parse) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.20.0->llama-cloud==0.1.34->llama-cloud-services>=0.6.49->llama-parse) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.20.0->llama-cloud==0.1.34->llama-cloud-services>=0.6.49->llama-parse) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.20.0->llama-cloud==0.1.34->llama-cloud-services>=0.6.49->llama-parse) (0.16.0)\n",
            "Requirement already satisfied: llama-index-instrumentation>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-workflows<2,>=1.0.1->llama-index-core>=0.12.0->llama-cloud-services>=0.6.49->llama-parse) (0.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core>=0.12.0->llama-cloud-services>=0.6.49->llama-parse) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core>=0.12.0->llama-cloud-services>=0.6.49->llama-parse) (2024.11.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core>=0.12.0->llama-cloud-services>=0.6.49->llama-parse) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core>=0.12.0->llama-cloud-services>=0.6.49->llama-parse) (2.4.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core>=0.12.0->llama-cloud-services>=0.6.49->llama-parse) (3.2.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect>=0.8.0->llama-index-core>=0.12.0->llama-cloud-services>=0.6.49->llama-parse) (1.1.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->llama-index-core>=0.12.0->llama-cloud-services>=0.6.49->llama-parse) (3.26.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.11/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core>=0.12.0->llama-cloud-services>=0.6.49->llama-parse) (25.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.20.0->llama-cloud==0.1.34->llama-cloud-services>=0.6.49->llama-parse) (1.3.1)\n",
            "Requirement already satisfied: colorama>=0.4 in /usr/local/lib/python3.11/dist-packages (from griffe->banks<3,>=2.0.0->llama-index-core>=0.12.0->llama-cloud-services>=0.6.49->llama-parse) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->banks<3,>=2.0.0->llama-index-core>=0.12.0->llama-cloud-services>=0.6.49->llama-parse) (3.0.2)\n",
            "Downloading llama_parse-0.6.50-py3-none-any.whl (4.9 kB)\n",
            "Downloading llama_cloud_services-0.6.51-py3-none-any.whl (48 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_cloud-0.1.34-py3-none-any.whl (289 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: python-dotenv, llama-cloud, llama-cloud-services, llama-parse\n",
            "Successfully installed llama-cloud-0.1.34 llama-cloud-services-0.6.51 llama-parse-0.6.50 python-dotenv-1.1.1\n",
            "Collecting llama-index-embeddings-openai\n",
            "  Downloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl.metadata (684 bytes)\n",
            "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-embeddings-openai) (0.12.50)\n",
            "Requirement already satisfied: openai>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-embeddings-openai) (1.97.0)\n",
            "Requirement already satisfied: aiohttp<4,>=3.8.6 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (3.11.15)\n",
            "Requirement already satisfied: aiosqlite in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (0.21.0)\n",
            "Requirement already satisfied: banks<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (2.2.0)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (1.2.18)\n",
            "Requirement already satisfied: dirtyjson<2,>=1.0.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (1.0.8)\n",
            "Requirement already satisfied: filetype<2,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (1.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (2025.3.2)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (0.28.1)\n",
            "Requirement already satisfied: llama-index-workflows<2,>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (1.1.0)\n",
            "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (3.5)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (2.0.2)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (11.2.1)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (4.3.8)\n",
            "Requirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (2.11.7)\n",
            "Requirement already satisfied: pyyaml>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=80.9.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (80.9.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (2.0.41)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (8.5.0)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (0.9.0)\n",
            "Requirement already satisfied: tqdm<5,>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (4.14.1)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (1.17.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.1.0->llama-index-embeddings-openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.1.0->llama-index-embeddings-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.1.0->llama-index-embeddings-openai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.1.0->llama-index-embeddings-openai) (1.3.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (1.20.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai>=1.1.0->llama-index-embeddings-openai) (3.10)\n",
            "Requirement already satisfied: griffe in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.0.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (1.7.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.0.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (3.1.6)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (0.16.0)\n",
            "Requirement already satisfied: llama-index-instrumentation>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-workflows<2,>=1.0.1->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (0.3.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (2024.11.6)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (2.4.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (3.2.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (1.1.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (3.26.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.11/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (25.0)\n",
            "Requirement already satisfied: colorama>=0.4 in /usr/local/lib/python3.11/dist-packages (from griffe->banks<3,>=2.0.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->banks<3,>=2.0.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (3.0.2)\n",
            "Downloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl (6.2 kB)\n",
            "Installing collected packages: llama-index-embeddings-openai\n",
            "Successfully installed llama-index-embeddings-openai-0.3.1\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "33ea01913e944a949e437e0521360bee",
              "pip_warning": {
                "packages": [
                  "llama_index"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting llama-index-readers-whisper\n",
            "  Downloading llama_index_readers_whisper-0.1.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-whisper) (0.12.50)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.28.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-whisper) (1.97.0)\n",
            "Requirement already satisfied: aiohttp<4,>=3.8.6 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-whisper) (3.11.15)\n",
            "Requirement already satisfied: aiosqlite in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-whisper) (0.21.0)\n",
            "Requirement already satisfied: banks<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-whisper) (2.2.0)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-whisper) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-whisper) (1.2.18)\n",
            "Requirement already satisfied: dirtyjson<2,>=1.0.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-whisper) (1.0.8)\n",
            "Requirement already satisfied: filetype<2,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-whisper) (1.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-whisper) (2025.3.2)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-whisper) (0.28.1)\n",
            "Requirement already satisfied: llama-index-workflows<2,>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-whisper) (1.1.0)\n",
            "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-whisper) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-whisper) (3.5)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-whisper) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-whisper) (2.0.2)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-whisper) (11.2.1)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-whisper) (4.3.8)\n",
            "Requirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-whisper) (2.11.7)\n",
            "Requirement already satisfied: pyyaml>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-whisper) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-whisper) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=80.9.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-whisper) (80.9.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-whisper) (2.0.41)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-whisper) (8.5.0)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-whisper) (0.9.0)\n",
            "Requirement already satisfied: tqdm<5,>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-whisper) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-whisper) (4.14.1)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-whisper) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-whisper) (1.17.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.28.0->llama-index-readers-whisper) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.28.0->llama-index-readers-whisper) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.28.0->llama-index-readers-whisper) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.28.0->llama-index-readers-whisper) (1.3.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-whisper) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-whisper) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-whisper) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-whisper) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-whisper) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-whisper) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-whisper) (1.20.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.28.0->llama-index-readers-whisper) (3.10)\n",
            "Requirement already satisfied: griffe in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.0.0->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-whisper) (1.7.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.0.0->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-whisper) (3.1.6)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-whisper) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-whisper) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-whisper) (0.16.0)\n",
            "Requirement already satisfied: llama-index-instrumentation>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-workflows<2,>=1.0.1->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-whisper) (0.3.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-whisper) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-whisper) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-whisper) (2024.11.6)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-whisper) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-whisper) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-whisper) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-whisper) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-whisper) (2.4.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-whisper) (3.2.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-whisper) (1.1.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-whisper) (3.26.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.11/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-whisper) (25.0)\n",
            "Requirement already satisfied: colorama>=0.4 in /usr/local/lib/python3.11/dist-packages (from griffe->banks<3,>=2.0.0->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-whisper) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->banks<3,>=2.0.0->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-whisper) (3.0.2)\n",
            "Downloading llama_index_readers_whisper-0.1.0-py3-none-any.whl (3.3 kB)\n",
            "Installing collected packages: llama-index-readers-whisper\n",
            "Successfully installed llama-index-readers-whisper-0.1.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "b590acbd8b4d454b977ff1e6a4e48a34",
              "pip_warning": {
                "packages": [
                  "llama_index"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.37.0)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.1.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.116.1)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.6.0)\n",
            "Requirement already satisfied: gradio-client==1.10.4 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.10.4)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.33.4)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.11.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (25.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.2.1)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.7)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.12.4)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.47.1)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.14.1)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.35.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.4->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.4->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1.0,>=0.24.1->gradio) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (1.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.4.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install llama-index-core\n",
        "!pip install llama-index-utils-workflow\n",
        "!pip install llama-index-llms-openai\n",
        "!pip install llama-parse\n",
        "!pip install llama-index-embeddings-openai\n",
        "!pip install llama-index-readers-whisper\n",
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1229da0-c307-47ff-a079-dbaaded2ea96",
      "metadata": {
        "id": "b1229da0-c307-47ff-a079-dbaaded2ea96"
      },
      "source": [
        "## Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de54d270-81bf-422a-aeb8-f1e83f2153d2",
      "metadata": {
        "height": 98,
        "id": "de54d270-81bf-422a-aeb8-f1e83f2153d2"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, HTML\n",
        "from llama_index.utils.workflow import draw_all_possible_flows\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "584e4be1-f8f1-4fcf-90a6-5b70a8567a57",
      "metadata": {
        "id": "584e4be1-f8f1-4fcf-90a6-5b70a8567a57"
      },
      "source": [
        "You need nested async for this to work, so let's enable it here. It allows you to nest asyncio event loops within each other.\n",
        "\n",
        "*Note:* In asynchronous programming, the event loop is like a continuous cycle that manages the execution of code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61267ae9-a841-47f1-9584-437fd1be5816",
      "metadata": {
        "height": 47,
        "id": "61267ae9-a841-47f1-9584-437fd1be5816"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23bee788-6706-4cb9-84e6-35fa3daac098",
      "metadata": {
        "id": "23bee788-6706-4cb9-84e6-35fa3daac098"
      },
      "source": [
        "You also need two API keys:\n",
        "- OpenAI like you used earlier;\n",
        "- LlamaCloud API key to use LlamaParse to parse the PDFs. In this notebook, you are provided with such a key. For your personal project, you can get a key at cloud.llamaindex.ai for free.\n",
        "\n",
        "LlamaParse is an advanced document parser that can read PDFs, Word files, Powerpoints, Excel spreadsheets, and extract information out of complicated PDFs into a form LLMs find easy to understand."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PfxEjN2J9XJ_",
      "metadata": {
        "id": "PfxEjN2J9XJ_"
      },
      "outputs": [],
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = \"your_key\"\n",
        "os.environ[\"LLAMA_CLOUD_API_KEY\"]=\"your_key\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ct_4jHfJ9XQ2",
      "metadata": {
        "id": "Ct_4jHfJ9XQ2"
      },
      "outputs": [],
      "source": [
        "def extract_html_content(filename):\n",
        "    try:\n",
        "        with open(filename, 'r') as file:\n",
        "            html_content = file.read()\n",
        "            html_content = f\"\"\" <div style=\"width: 100%; height: 800px; overflow: hidden;\"> {html_content} </div>\"\"\"\n",
        "            return html_content\n",
        "    except Exception as e:\n",
        "        raise Exception(f\"Error reading file: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa144829-78b4-409b-a9f7-3b4e71b28e1e",
      "metadata": {
        "id": "aa144829-78b4-409b-a9f7-3b4e71b28e1e"
      },
      "source": [
        "## Performing Retrieval-Augmented Generation (RAG) on a Resume Document"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e19853ba-dc5e-44e2-9395-9c2db18d22c9",
      "metadata": {
        "id": "e19853ba-dc5e-44e2-9395-9c2db18d22c9"
      },
      "source": [
        "### 1. Parsing the Resume Document"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e123386-f058-4ab3-a2a2-aeed9ec90fb4",
      "metadata": {
        "id": "3e123386-f058-4ab3-a2a2-aeed9ec90fb4"
      },
      "source": [
        "Let's start by parsing a resume.\n",
        "\n",
        "<img width=\"400\" src=\"images/parsing_res.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "744ef68c-2ffd-41a2-bd50-46fe4bea35ba",
      "metadata": {
        "id": "744ef68c-2ffd-41a2-bd50-46fe4bea35ba"
      },
      "source": [
        "Using LLamaParse, you will transform the resume into a list of Document objects. By default, a Document object stores text along with some other attributes:\n",
        "- metadata: a dictionary of annotations that can be appended to the text.\n",
        "- relationships: a dictionary containing relationships to other Documents.\n",
        "  \n",
        "\n",
        "You can tell LlamaParse what kind of document it's parsing, so that it will parse the contents more intelligently. In this case, you tell it that it's reading a resume."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86ccb487-d089-4ac2-844a-4b3bc0f5e223",
      "metadata": {
        "height": 30,
        "id": "86ccb487-d089-4ac2-844a-4b3bc0f5e223"
      },
      "outputs": [],
      "source": [
        "from llama_parse import LlamaParse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ff32431-3ae4-446a-a458-c40949ef6f4f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "height": 164,
        "id": "3ff32431-3ae4-446a-a458-c40949ef6f4f",
        "outputId": "d86c3ae3-7ddd-4cb5-9552-852860789737"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING: content_guideline_instruction is deprecated and may be remove in a future release. Use system_prompt, system_prompt_append or user_prompt instead.\n",
            "Started parsing the file under job_id 106d82f1-67c4-430b-95f0-d2a263019861\n"
          ]
        }
      ],
      "source": [
        "documents = LlamaParse(\n",
        "    api_key=os.getenv(\"LLAMA_CLOUD_API_KEY\"),\n",
        "    base_url=os.getenv(\"LLAMA_CLOUD_BASE_URL\"),\n",
        "    result_type=\"markdown\",\n",
        "    content_guideline_instruction=\"This is a resume, gather related facts together and format it as bullet points with headers\"\n",
        ").load_data(\n",
        "    \"/content/fake_resume.pdf\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e0fdeea-70ee-4422-a488-003a305bcf5a",
      "metadata": {
        "id": "9e0fdeea-70ee-4422-a488-003a305bcf5a"
      },
      "source": [
        "This gives you a list of Document objects you can feed to a VectorStoreIndex."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1245b406-c0ff-42b8-9498-cb14e2f729bf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "height": 30,
        "id": "1245b406-c0ff-42b8-9498-cb14e2f729bf",
        "outputId": "56099849-9e7c-4669-b053-e81a3456968a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# Projects\n",
            "\n",
            "# EcoTrack | GitHub\n",
            "\n",
            "- Built full-stack application for tracking carbon footprint using React, Node.js, and MongoDB\n",
            "- Implemented machine learning algorithm for providing personalized sustainability recommendations\n",
            "- Featured in TechCrunch's \"Top 10 Environmental Impact Apps of 2023\"\n",
            "\n",
            "# ChatFlow | Demo\n",
            "\n",
            "- Developed real-time chat application using WebSocket protocol and React\n",
            "- Implemented end-to-end encryption and message persistence\n",
            "- Serves 5000+ monthly active users\n",
            "\n",
            "# Certifications\n",
            "\n",
            "- AWS Certified Solutions Architect (2023)\n",
            "- Google Cloud Professional Developer (2022)\n",
            "- MongoDB Certified Developer (2021)\n",
            "\n",
            "# Languages\n",
            "\n",
            "- English (Native)\n",
            "- Mandarin Chinese (Fluent)\n",
            "- Spanish (Intermediate)\n",
            "\n",
            "# Interests\n",
            "\n",
            "- Open source contribution\n",
            "- Tech blogging (15K+ Medium followers)\n",
            "- Hackathon mentoring\n",
            "- Rock climbing\n"
          ]
        }
      ],
      "source": [
        "print(documents[2].text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86df6497-f6b0-4231-8fc2-57a326f2bac6",
      "metadata": {
        "id": "86df6497-f6b0-4231-8fc2-57a326f2bac6"
      },
      "source": [
        "### 2. Creating a Vector Store Index"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "869382b7-4077-4eb7-81df-a262831c4ebe",
      "metadata": {
        "id": "869382b7-4077-4eb7-81df-a262831c4ebe"
      },
      "source": [
        "\n",
        "<img width=\"400\" src=\"images/vector_store_index.png\">\n",
        "\n",
        "You'll now feed the Document objects to `VectorStoreIndex`. The `VectorStoreIndex` will use an embedding model to embed the text, i.e. turn it into vectors that you can search. You'll be using an embedding model provided by OpenAI, which is why we needed an OpenAI key.\n",
        "\n",
        "The `VectorStoreIndex` will return an index object, which is a data structure that allows you to quickly retrieve relevant context for your query. It's the core foundation for RAG use-cases. You can use indexes to build Query Engines and Chat Engines which enables question & answer and chat over your data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90d6351c-9e1f-457b-b6d0-d739fb826743",
      "metadata": {
        "height": 47,
        "id": "90d6351c-9e1f-457b-b6d0-d739fb826743"
      },
      "outputs": [],
      "source": [
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.core import VectorStoreIndex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b6b96c5-82a7-4c9f-9e8d-300806533340",
      "metadata": {
        "height": 98,
        "id": "4b6b96c5-82a7-4c9f-9e8d-300806533340"
      },
      "outputs": [],
      "source": [
        "index = VectorStoreIndex.from_documents(\n",
        "    documents,\n",
        "    embed_model=OpenAIEmbedding(model_name=\"text-embedding-3-small\",\n",
        "                                api_key= os.getenv(\"OPENAI_API_KEY\"))\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01de6653-243b-4ddd-8a1a-efd77dfbb1db",
      "metadata": {
        "id": "01de6653-243b-4ddd-8a1a-efd77dfbb1db"
      },
      "source": [
        "### 3. Creating a Query Engine with the Index"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da4460b2-7c72-4944-80f4-642e437d7eff",
      "metadata": {
        "id": "da4460b2-7c72-4944-80f4-642e437d7eff"
      },
      "source": [
        "With an index, you can create a query engine and ask questions. Let's try it out! Asking questions requires an LLM, so let's use OpenAI again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "956a7e3b-7861-4af2-a9c9-9613606b53c6",
      "metadata": {
        "height": 30,
        "id": "956a7e3b-7861-4af2-a9c9-9613606b53c6"
      },
      "outputs": [],
      "source": [
        "from llama_index.llms.openai import OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97399cf0-ac3a-43c0-a7d7-fb56df5fc3dc",
      "metadata": {
        "height": 30,
        "id": "97399cf0-ac3a-43c0-a7d7-fb56df5fc3dc"
      },
      "outputs": [],
      "source": [
        "llm = OpenAI(model=\"gpt-4o-mini\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8820d8b0-4749-4414-afbe-cbc3bc6146dd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "height": 79,
        "id": "8820d8b0-4749-4414-afbe-cbc3bc6146dd",
        "outputId": "b8d9a953-3ec5-47b2-f32c-514de83fa49b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The person's name is Sarah Chen, and their most recent job is Senior Full Stack Developer at TechFlow Solutions in San Francisco, CA.\n"
          ]
        }
      ],
      "source": [
        "query_engine = index.as_query_engine(llm=llm, similarity_top_k=5)\n",
        "response = query_engine.query(\"What is this person's name and what was their most recent job?\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d443cdeb-fb24-4ad9-abd5-c4bf845ca258",
      "metadata": {
        "id": "d443cdeb-fb24-4ad9-abd5-c4bf845ca258"
      },
      "source": [
        "### 4. Storing the Index to Disk"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b07b7836-90cf-4883-a9bc-18fb7a443023",
      "metadata": {
        "id": "b07b7836-90cf-4883-a9bc-18fb7a443023"
      },
      "source": [
        "Indexes can be persisted to disk. This is useful in a notebook that you might run several times! In a production setting, you would probably use a hosted vector store of some kind. Let's save your index to disk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6dc5fbb0-e700-45d9-9e44-a6deae557a94",
      "metadata": {
        "height": 64,
        "id": "6dc5fbb0-e700-45d9-9e44-a6deae557a94"
      },
      "outputs": [],
      "source": [
        "storage_dir = \"/content/storage\"\n",
        "\n",
        "index.storage_context.persist(persist_dir=storage_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e0d88ae-c97d-42d9-a67c-1e3e846a4e11",
      "metadata": {
        "height": 30,
        "id": "7e0d88ae-c97d-42d9-a67c-1e3e846a4e11"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import StorageContext, load_index_from_storage"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24ffb6d9-aa9d-4f5b-a743-5c3a0e4dafc9",
      "metadata": {
        "id": "24ffb6d9-aa9d-4f5b-a743-5c3a0e4dafc9"
      },
      "source": [
        "You can check if your index has already been stored, and if it has, you can reload an index from disk using the `load_index_from_storage` method, like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "733c762c-27d0-4bab-9a72-c5c0e568f9c4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "height": 147,
        "id": "733c762c-27d0-4bab-9a72-c5c0e568f9c4",
        "outputId": "8de85e06-7e12-4300-8233-5ab9b3dc1637"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading llama_index.core.storage.kvstore.simple_kvstore from /content/storage/docstore.json.\n",
            "Loading llama_index.core.storage.kvstore.simple_kvstore from /content/storage/index_store.json.\n"
          ]
        }
      ],
      "source": [
        "# Check if the index is stored on disk\n",
        "if os.path.exists(storage_dir):\n",
        "    # Load the index from disk\n",
        "    storage_context = StorageContext.from_defaults(persist_dir=storage_dir)\n",
        "    restored_index = load_index_from_storage(storage_context)\n",
        "else:\n",
        "    print(\"Index not found on disk.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "933e1172-2187-4847-a5a6-fca67eb9f1d6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "height": 62,
        "id": "933e1172-2187-4847-a5a6-fca67eb9f1d6",
        "outputId": "8bb59e5a-e8fc-4da1-904b-048772c63ef0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This person's name is Sarah Chen and their most recent job was as a Senior Full Stack Developer at TechFlow Solutions in San Francisco, CA.\n"
          ]
        }
      ],
      "source": [
        "response = restored_index.as_query_engine().query(\"What is this person's name and what was their most recent job?\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0077027c-d4de-4627-9b35-66362039ad54",
      "metadata": {
        "id": "0077027c-d4de-4627-9b35-66362039ad54"
      },
      "source": [
        "Congratulations! You have performed retrieval augmented generation (RAG) on a resume document. With proper scaling, this technique can work across databases of thousands of documents!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a709fda1-7cd5-4c9a-b219-ed64d16d9e76",
      "metadata": {
        "id": "a709fda1-7cd5-4c9a-b219-ed64d16d9e76"
      },
      "source": [
        "## Making RAG Agentic"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba0d7e4b-5167-4214-a09b-132c62c5370b",
      "metadata": {
        "id": "ba0d7e4b-5167-4214-a09b-132c62c5370b"
      },
      "source": [
        "With a RAG pipeline in hand, let's turn it into a tool that can be used by an agent to answer questions. This is a stepping-stone towards creating an agentic system that can perform your larger goal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30f1ca1b-7e2f-416b-aa42-1adcd7d0fd53",
      "metadata": {
        "height": 47,
        "id": "30f1ca1b-7e2f-416b-aa42-1adcd7d0fd53"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.tools import FunctionTool\n",
        "from llama_index.core.agent import FunctionCallingAgent"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2a3ce49-7009-4bb4-a8e7-8f3d543a9ca2",
      "metadata": {
        "id": "f2a3ce49-7009-4bb4-a8e7-8f3d543a9ca2"
      },
      "source": [
        "First, create a regular python function that performs a RAG query. It's important to give this function a descriptive name, to mark its input and output types, and to include a docstring (that's the thing in triple quotes) which describes what it does. The framework will give all this metadata to the LLM, which will use it to decide what a tool does and whether to use it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6837a256-7273-4a63-9f0b-2733a9f143d8",
      "metadata": {
        "height": 113,
        "id": "6837a256-7273-4a63-9f0b-2733a9f143d8"
      },
      "outputs": [],
      "source": [
        "def query_resume(q: str) -> str:\n",
        "    \"\"\"Answers questions about a specific resume.\"\"\"\n",
        "    # we're using the query engine we already created above\n",
        "    response = query_engine.query(f\"This is a question about the specific resume we have in our database: {q}\")\n",
        "    return response.response"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f0b3802-0452-49f7-a6b2-19d173f18021",
      "metadata": {
        "id": "6f0b3802-0452-49f7-a6b2-19d173f18021"
      },
      "source": [
        "The next step is to create the actual tool. There's a utility function, `FunctionTool.from_defaults`, to do this for you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "507c1f77-3f9c-4864-ae8c-2fcbc4f91a30",
      "metadata": {
        "height": 30,
        "id": "507c1f77-3f9c-4864-ae8c-2fcbc4f91a30"
      },
      "outputs": [],
      "source": [
        "resume_tool = FunctionTool.from_defaults(fn=query_resume)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0e0b483-dcb0-4b65-b3ba-0311f6e41766",
      "metadata": {
        "id": "f0e0b483-dcb0-4b65-b3ba-0311f6e41766"
      },
      "source": [
        "Now you can instantiate a `FunctionCallingAgent` using that tool. There are a number of different agent types supported by LlamaIndex; this one is particularly capable and efficient.\n",
        "\n",
        "You pass it an array of tools (just one in this case), you give it the same LLM we instantiated earlier, and you set Verbose to true so you get a little more info on what your agent is up to."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53a2b4bb-f4ef-494a-af61-e12052706945",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "height": 98,
        "id": "53a2b4bb-f4ef-494a-af61-e12052706945",
        "outputId": "f2dddd2c-2f6c-42c1-b7dd-df4ddc55e0db"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/llama_index/core/agent/function_calling/base.py:87: DeprecationWarning: Call to deprecated class FunctionCallingAgent. (FunctionCallingAgent has been rewritten and replaced by newer agents based on llama_index.core.agent.workflow.FunctionAgent.\n",
            "\n",
            "This implementation will be removed in a v0.13.0.\n",
            "\n",
            "See the docs for more information on updated usage: https://docs.llamaindex.ai/en/stable/understanding/agent/)\n",
            "  return cls(\n",
            "/usr/local/lib/python3.11/dist-packages/deprecated/classic.py:184: DeprecationWarning: Call to deprecated class AgentRunner. (AgentRunner has been deprecated and is not maintained.\n",
            "\n",
            "This implementation will be removed in a v0.13.0.\n",
            "\n",
            "See the docs for more information on updated agent usage: https://docs.llamaindex.ai/en/stable/understanding/agent/)\n",
            "  return old_new1(cls, *args, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "agent = FunctionCallingAgent.from_tools(\n",
        "    tools=[resume_tool],\n",
        "    llm=llm,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b603f59f-dc5a-47fe-b0d8-0360e49cb9e6",
      "metadata": {
        "id": "b603f59f-dc5a-47fe-b0d8-0360e49cb9e6"
      },
      "source": [
        "Now you can chat to the agent! Let's ask it a quick question about our applicant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22722aa1-dcb0-4225-a16e-8ccdf254f288",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "height": 62,
        "id": "22722aa1-dcb0-4225-a16e-8ccdf254f288",
        "outputId": "2870aed0-0f68-4b2c-fa04-9c93174d1f8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> Running step 09da25cd-4168-4c0a-899c-5acffa6244fd. Step input: How many years of experience does the applicant have?\n",
            "Added user message to memory: How many years of experience does the applicant have?\n",
            "=== Calling Function ===\n",
            "Calling function: query_resume with args: {\"q\": \"How many years of experience does the applicant have?\"}\n",
            "=== Function Output ===\n",
            "The applicant has over 6 years of experience in web development.\n",
            "> Running step f7cda5e8-0032-453e-88d2-815039f30da7. Step input: None\n",
            "=== LLM Response ===\n",
            "The applicant has over 6 years of experience in web development.\n",
            "The applicant has over 6 years of experience in web development.\n"
          ]
        }
      ],
      "source": [
        "response = agent.chat(\"How many years of experience does the applicant have?\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88192e1d-b3a4-4e9a-aa21-35681997c82a",
      "metadata": {
        "id": "88192e1d-b3a4-4e9a-aa21-35681997c82a"
      },
      "source": [
        "You can see the agent getting the question, adding it to its memory, picking a tool, calling it with appropriate arguments, and getting the output back."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "751e4848-c143-434d-8c00-587435daa29d",
      "metadata": {
        "id": "751e4848-c143-434d-8c00-587435daa29d"
      },
      "source": [
        "## Wrapping the Agentic RAG into a Workflow"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50080f70-b004-4a61-b2e3-8a0238f2b972",
      "metadata": {
        "id": "50080f70-b004-4a61-b2e3-8a0238f2b972"
      },
      "source": [
        "You've now got a RAG pipeline and an agent. Let's now create a similar agentic RAG from scratch using a workflow, which you'll extend in later lessons. You won't rely on any of the things you've already created.\n",
        "\n",
        "Here's the workflow you will create:\n",
        "<img width=\"400\" src=\"images/rag_workflow.png\">\n",
        "\n",
        "It consists of two steps:\n",
        "1. `set_up` which is triggered by `StartEvent` and emits `QueryEvent`: at this step, the RAG system is set up and the query is passed to the second step;\n",
        "2. `ask_question` which is triggered by `QueryEvent` and emits `StopEvent`: here the response to the query is generated using the RAG query engine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "785f156c-9f06-4ea9-90cc-2f629fc7a7c8",
      "metadata": {
        "height": 149,
        "id": "785f156c-9f06-4ea9-90cc-2f629fc7a7c8"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.workflow import (\n",
        "    StartEvent,\n",
        "    StopEvent,\n",
        "    Workflow,\n",
        "    step,\n",
        "    Event,\n",
        "    Context\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "659595b3-714f-441a-bd5b-84c36ea76367",
      "metadata": {
        "height": 47,
        "id": "659595b3-714f-441a-bd5b-84c36ea76367"
      },
      "outputs": [],
      "source": [
        "class QueryEvent(Event):\n",
        "    query: str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06f76078-eb50-48d9-af22-47d5504e7e78",
      "metadata": {
        "height": 776,
        "id": "06f76078-eb50-48d9-af22-47d5504e7e78"
      },
      "outputs": [],
      "source": [
        "class RAGWorkflow(Workflow):\n",
        "    storage_dir = \"/content/storage\"\n",
        "    llm: OpenAI\n",
        "    query_engine: VectorStoreIndex\n",
        "\n",
        "    # the first step will be setup\n",
        "    @step\n",
        "    async def set_up(self, ctx: Context, ev: StartEvent) -> QueryEvent:\n",
        "\n",
        "        if not ev.resume_file:\n",
        "            raise ValueError(\"No resume file provided\")\n",
        "\n",
        "        # define an LLM to work with\n",
        "        self.llm = OpenAI(model=\"gpt-4o-mini\")\n",
        "\n",
        "        # ingest the data and set up the query engine\n",
        "        if os.path.exists(self.storage_dir):\n",
        "            # you've already ingested your documents\n",
        "            storage_context = StorageContext.from_defaults(persist_dir=self.storage_dir)\n",
        "            index = load_index_from_storage(storage_context)\n",
        "        else:\n",
        "            # parse and load your documents\n",
        "            documents = LlamaParse(\n",
        "                result_type=\"markdown\",\n",
        "                content_guideline_instruction=\"This is a resume, gather related facts together and format it as bullet points with headers\"\n",
        "            ).load_data(ev.resume_file)\n",
        "            # embed and index the documents\n",
        "            index = VectorStoreIndex.from_documents(\n",
        "                documents,\n",
        "                embed_model=OpenAIEmbedding(model_name=\"text-embedding-3-small\")\n",
        "            )\n",
        "            index.storage_context.persist(persist_dir=self.storage_dir)\n",
        "\n",
        "        # either way, create a query engine\n",
        "        self.query_engine = index.as_query_engine(llm=self.llm, similarity_top_k=5)\n",
        "\n",
        "        # now fire off a query event to trigger the next step\n",
        "        return QueryEvent(query=ev.query)\n",
        "\n",
        "    # the second step will be to ask a question and return a result immediately\n",
        "    @step\n",
        "    async def ask_question(self, ctx: Context, ev: QueryEvent) -> StopEvent:\n",
        "        response = self.query_engine.query(f\"This is a question about the specific resume we have in our database: {ev.query}\")\n",
        "        return StopEvent(result=response.response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63bf8325-7f79-4855-8080-a7df1237929b",
      "metadata": {
        "id": "63bf8325-7f79-4855-8080-a7df1237929b"
      },
      "source": [
        "You run it like before, giving it a fake resume we created for you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a06c2823-e2ae-4bc0-990d-e23707df7804",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "height": 115,
        "id": "a06c2823-e2ae-4bc0-990d-e23707df7804",
        "outputId": "1f4c6129-ebdf-4467-dec4-df182fe978d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading llama_index.core.storage.kvstore.simple_kvstore from /content/storage/docstore.json.\n",
            "Loading llama_index.core.storage.kvstore.simple_kvstore from /content/storage/index_store.json.\n",
            "The first place the applicant worked is StartupHub in San Jose, CA.\n"
          ]
        }
      ],
      "source": [
        "w = RAGWorkflow(timeout=120, verbose=False)\n",
        "result = await w.run(\n",
        "    resume_file=\"/content/fake_resume.pdf\",\n",
        "    query=\"Where is the first place the applicant worked?\"\n",
        ")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09770f81-5465-4407-b2f3-885dd6a2dc6f",
      "metadata": {
        "id": "09770f81-5465-4407-b2f3-885dd6a2dc6f"
      },
      "source": [
        "There's nothing in this workflow you haven't done before, it's just making things neat and encapsulated.\n",
        "\n",
        "If you're particularly suspicious, you might notice there's a small bug here: if you run this a second time, with a new resume, this code will find the old resume and not bother to parse it. You don't need to fix that now, but think about how you might fix that."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8673e982",
      "metadata": {
        "id": "8673e982"
      },
      "source": [
        "## Workflow Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b48c5f32-a579-444f-8199-618a1a34b696",
      "metadata": {
        "id": "b48c5f32-a579-444f-8199-618a1a34b696"
      },
      "source": [
        "You can visualize the workflow you just created."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "460b825b-d395-4c7a-ba78-635cddd4ff02",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 842,
          "resources": {
            "http://localhost:8080/lib/bindings/utils.js": {
              "data": "",
              "headers": [
                [
                  "content-length",
                  "0"
                ]
              ],
              "ok": false,
              "status": 404,
              "status_text": ""
            }
          }
        },
        "height": 81,
        "id": "460b825b-d395-4c7a-ba78-635cddd4ff02",
        "outputId": "610993cd-1993-4745-d81f-05136749b921"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/workflows/rag_workflow.html\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " <div style=\"width: 100%; height: 800px; overflow: hidden;\"> <html>\n",
              "    <head>\n",
              "        <meta charset=\"utf-8\">\n",
              "        \n",
              "            <script src=\"lib/bindings/utils.js\"></script>\n",
              "            <link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/dist/vis-network.min.css\" integrity=\"sha512-WgxfT5LWjfszlPHXRmBWHkV2eceiWTOBvrKCNbdgDYTHrT2AeLCGbF4sZlZw3UMN3WtL0tGUoIAKsu8mllg/XA==\" crossorigin=\"anonymous\" referrerpolicy=\"no-referrer\" />\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/vis-network.min.js\" integrity=\"sha512-LnvoEWDFrqGHlHmDD2101OrLcbsfkrzoSpvtSQtxK3RMnRV0eOkhhBN2dXHKRrUU8p2DGRTk35n4O8nWSVe1mQ==\" crossorigin=\"anonymous\" referrerpolicy=\"no-referrer\"></script>\n",
              "            \n",
              "        \n",
              "<center>\n",
              "<h1></h1>\n",
              "</center>\n",
              "\n",
              "<!-- <link rel=\"stylesheet\" href=\"../node_modules/vis/dist/vis.min.css\" type=\"text/css\" />\n",
              "<script type=\"text/javascript\" src=\"../node_modules/vis/dist/vis.js\"> </script>-->\n",
              "        <link\n",
              "          href=\"https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/css/bootstrap.min.css\"\n",
              "          rel=\"stylesheet\"\n",
              "          integrity=\"sha384-eOJMYsd53ii+scO/bJGFsiCZc+5NDVN2yr8+0RDqr0Ql0h+rP48ckxlpbzKgwra6\"\n",
              "          crossorigin=\"anonymous\"\n",
              "        />\n",
              "        <script\n",
              "          src=\"https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/js/bootstrap.bundle.min.js\"\n",
              "          integrity=\"sha384-JEW9xMcG8R+pH31jmWH6WWP0WintQrMb4s7ZOdauHnUtxwoG2vI5DkLtS3qm9Ekf\"\n",
              "          crossorigin=\"anonymous\"\n",
              "        ></script>\n",
              "\n",
              "\n",
              "        <center>\n",
              "          <h1></h1>\n",
              "        </center>\n",
              "        <style type=\"text/css\">\n",
              "\n",
              "             #mynetwork {\n",
              "                 width: 100%;\n",
              "                 height: 750px;\n",
              "                 background-color: #ffffff;\n",
              "                 border: 1px solid lightgray;\n",
              "                 position: relative;\n",
              "                 float: left;\n",
              "             }\n",
              "\n",
              "             \n",
              "\n",
              "             \n",
              "\n",
              "             \n",
              "        </style>\n",
              "    </head>\n",
              "\n",
              "\n",
              "    <body>\n",
              "        <div class=\"card\" style=\"width: 100%\">\n",
              "            \n",
              "            \n",
              "            <div id=\"mynetwork\" class=\"card-body\"></div>\n",
              "        </div>\n",
              "\n",
              "        \n",
              "        \n",
              "\n",
              "        <script type=\"text/javascript\">\n",
              "\n",
              "              // initialize global variables.\n",
              "              var edges;\n",
              "              var nodes;\n",
              "              var allNodes;\n",
              "              var allEdges;\n",
              "              var nodeColors;\n",
              "              var originalNodes;\n",
              "              var network;\n",
              "              var container;\n",
              "              var options, data;\n",
              "              var filter = {\n",
              "                  item : '',\n",
              "                  property : '',\n",
              "                  value : []\n",
              "              };\n",
              "\n",
              "              \n",
              "\n",
              "              \n",
              "\n",
              "              // This method is responsible for drawing the graph, returns the drawn network\n",
              "              function drawGraph() {\n",
              "                  var container = document.getElementById('mynetwork');\n",
              "\n",
              "                  \n",
              "\n",
              "                  // parsing and collecting nodes and edges from the python\n",
              "                  nodes = new vis.DataSet([{\"color\": \"#ADD8E6\", \"id\": \"_done\", \"label\": \"_done\", \"shape\": \"box\", \"title\": null}, {\"color\": \"#FFA07A\", \"id\": \"StopEvent\", \"label\": \"StopEvent\", \"shape\": \"ellipse\", \"title\": null}, {\"color\": \"#ADD8E6\", \"id\": \"ask_question\", \"label\": \"ask_question\", \"shape\": \"box\", \"title\": null}, {\"color\": \"#90EE90\", \"id\": \"QueryEvent\", \"label\": \"QueryEvent\", \"shape\": \"ellipse\", \"title\": null}, {\"color\": \"#ADD8E6\", \"id\": \"set_up\", \"label\": \"set_up\", \"shape\": \"box\", \"title\": null}, {\"color\": \"#E27AFF\", \"id\": \"StartEvent\", \"label\": \"StartEvent\", \"shape\": \"ellipse\", \"title\": null}]);\n",
              "                  edges = new vis.DataSet([{\"arrows\": \"to\", \"from\": \"StopEvent\", \"to\": \"_done\"}, {\"arrows\": \"to\", \"from\": \"ask_question\", \"to\": \"StopEvent\"}, {\"arrows\": \"to\", \"from\": \"QueryEvent\", \"to\": \"ask_question\"}, {\"arrows\": \"to\", \"from\": \"set_up\", \"to\": \"QueryEvent\"}, {\"arrows\": \"to\", \"from\": \"StartEvent\", \"to\": \"set_up\"}]);\n",
              "\n",
              "                  nodeColors = {};\n",
              "                  allNodes = nodes.get({ returnType: \"Object\" });\n",
              "                  for (nodeId in allNodes) {\n",
              "                    nodeColors[nodeId] = allNodes[nodeId].color;\n",
              "                  }\n",
              "                  allEdges = edges.get({ returnType: \"Object\" });\n",
              "                  // adding nodes and edges to the graph\n",
              "                  data = {nodes: nodes, edges: edges};\n",
              "\n",
              "                  var options = {\n",
              "    \"configure\": {\n",
              "        \"enabled\": false\n",
              "    },\n",
              "    \"edges\": {\n",
              "        \"color\": {\n",
              "            \"inherit\": true\n",
              "        },\n",
              "        \"smooth\": {\n",
              "            \"enabled\": true,\n",
              "            \"type\": \"dynamic\"\n",
              "        }\n",
              "    },\n",
              "    \"interaction\": {\n",
              "        \"dragNodes\": true,\n",
              "        \"hideEdgesOnDrag\": false,\n",
              "        \"hideNodesOnDrag\": false\n",
              "    },\n",
              "    \"physics\": {\n",
              "        \"enabled\": true,\n",
              "        \"stabilization\": {\n",
              "            \"enabled\": true,\n",
              "            \"fit\": true,\n",
              "            \"iterations\": 1000,\n",
              "            \"onlyDynamicEdges\": false,\n",
              "            \"updateInterval\": 50\n",
              "        }\n",
              "    }\n",
              "};\n",
              "\n",
              "                  \n",
              "\n",
              "\n",
              "                  \n",
              "\n",
              "                  network = new vis.Network(container, data, options);\n",
              "\n",
              "                  \n",
              "\n",
              "                  \n",
              "\n",
              "                  \n",
              "\n",
              "\n",
              "                  \n",
              "\n",
              "                  return network;\n",
              "\n",
              "              }\n",
              "              drawGraph();\n",
              "        </script>\n",
              "    </body>\n",
              "</html> </div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "isolated": true
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "WORKFLOW_FILE = \"/content/workflows/rag_workflow.html\"\n",
        "draw_all_possible_flows(w, filename=WORKFLOW_FILE)\n",
        "html_content = extract_html_content(WORKFLOW_FILE)\n",
        "display(HTML(html_content), metadata=dict(isolated=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ace7d2ad-998d-4fa9-9c34-588ed333441c",
      "metadata": {
        "id": "ace7d2ad-998d-4fa9-9c34-588ed333441c"
      },
      "source": [
        "## Congratulations!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "019261f7-f7b0-4729-b79a-56ff097c619f",
      "metadata": {
        "id": "019261f7-f7b0-4729-b79a-56ff097c619f"
      },
      "source": [
        "You've successfully created an agent with RAG tools. In the next lesson, you'll give your agent more complicated tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7zRtCh_qEcRz",
      "metadata": {
        "id": "7zRtCh_qEcRz"
      },
      "source": [
        "# **FORM PARSING**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QDw6tEy2EiOB",
      "metadata": {
        "id": "QDw6tEy2EiOB"
      },
      "outputs": [],
      "source": [
        "class ParseFormEvent(Event):\n",
        "    application_form: str\n",
        "\n",
        "class QueryEvent(Event):\n",
        "    query: str\n",
        "    field: str\n",
        "\n",
        "# new!\n",
        "class ResponseEvent(Event):\n",
        "    response: str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Kvd-zT-XI9hV",
      "metadata": {
        "id": "Kvd-zT-XI9hV"
      },
      "outputs": [],
      "source": [
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XIUs2wdKGUyL",
      "metadata": {
        "id": "XIUs2wdKGUyL"
      },
      "outputs": [],
      "source": [
        "class RAGWorkflow(Workflow):\n",
        "\n",
        "    storage_dir = \"/content/storage\"\n",
        "    llm: OpenAI\n",
        "    query_engine: VectorStoreIndex\n",
        "\n",
        "    @step\n",
        "    async def set_up(self, ctx: Context, ev: StartEvent) -> ParseFormEvent:\n",
        "\n",
        "        if not ev.resume_file:\n",
        "            raise ValueError(\"No resume file provided\")\n",
        "\n",
        "        if not ev.application_form:\n",
        "            raise ValueError(\"No application form provided\")\n",
        "\n",
        "        # define the LLM to work with\n",
        "        self.llm = OpenAI(model=\"gpt-4o-mini\")\n",
        "\n",
        "        # ingest the data and set up the query engine\n",
        "        if os.path.exists(self.storage_dir):\n",
        "            # you've already ingested the resume document\n",
        "            storage_context = StorageContext.from_defaults(persist_dir=self.storage_dir)\n",
        "            index = load_index_from_storage(storage_context)\n",
        "        else:\n",
        "            # parse and load the resume document\n",
        "            documents = LlamaParse(\n",
        "                api_key=os.getenv(\"LLAMA_CLOUD_API_KEY\"),\n",
        "                base_url=os.getenv(\"LLAMA_CLOUD_BASE_URL\"),\n",
        "                result_type=\"markdown\",\n",
        "                content_guideline_instruction=\"This is a resume, gather related facts together and format it as bullet points with headers\"\n",
        "            ).load_data(ev.resume_file)\n",
        "            # embed and index the documents\n",
        "            index = VectorStoreIndex.from_documents(\n",
        "                documents,\n",
        "                embed_model=OpenAIEmbedding(model_name=\"text-embedding-3-small\")\n",
        "            )\n",
        "            index.storage_context.persist(persist_dir=self.storage_dir)\n",
        "\n",
        "        # create a query engine\n",
        "        self.query_engine = index.as_query_engine(llm=self.llm, similarity_top_k=5)\n",
        "\n",
        "        # you no longer need a query to be passed in,\n",
        "        # you'll be generating the queries instead\n",
        "        # let's pass the application form to a new step to parse it\n",
        "        return ParseFormEvent(application_form=ev.application_form)\n",
        "\n",
        "    @step\n",
        "    async def parse_form(self, ctx: Context, ev: ParseFormEvent) -> QueryEvent:\n",
        "        print(\"Parsing the form...\")\n",
        "        parser = LlamaParse(\n",
        "            api_key=os.getenv(\"LLAMA_CLOUD_API_KEY\"),\n",
        "            base_url=os.getenv(\"LLAMA_CLOUD_BASE_URL\"),\n",
        "            result_type=\"markdown\",\n",
        "            content_guideline_instruction=\"This is a job application form. Create a list of all the fields that need to be filled in.\",\n",
        "            formatting_instruction=\"Return a bulleted list of the fields ONLY.\"\n",
        "        )\n",
        "\n",
        "        # get the LLM to convert the parsed form into JSON\n",
        "        result = parser.load_data(ev.application_form)[0]\n",
        "        raw_json = self.llm.complete(\n",
        "            f\"\"\"\n",
        "            This is a parsed form.\n",
        "            Convert it into a JSON object containing only the list\n",
        "            of fields to be filled in, in the form {{ fields: [...] }}.\n",
        "            <form>{result.text}</form>.\n",
        "            Return JSON ONLY, no markdown.\n",
        "            \"\"\")\n",
        "        fields = json.loads(raw_json.text)[\"fields\"]\n",
        "        print(f\"Fields: {fields}\")\n",
        "        # new!\n",
        "        # generate one query for each of the fields, and fire them off\n",
        "        for field in fields:\n",
        "            ctx.send_event(QueryEvent(\n",
        "                field=field,\n",
        "                query=f\"How would you answer this question about the candidate? {field}\"\n",
        "            ))\n",
        "\n",
        "        # store the number of fields so we know how many to wait for later\n",
        "        await ctx.set(\"total_fields\", len(fields))\n",
        "        return\n",
        "\n",
        "    @step\n",
        "    async def ask_question(self, ctx: Context, ev: QueryEvent) -> ResponseEvent:\n",
        "        response = self.query_engine.query(f\"This is a question about the specific resume we have in our database: {ev.query}\")\n",
        "        return ResponseEvent(field=ev.field, response=response.response)\n",
        "\n",
        "    # new!\n",
        "    @step\n",
        "    async def fill_in_application(self, ctx: Context, ev: ResponseEvent) -> StopEvent:\n",
        "        # get the total number of fields to wait for\n",
        "        total_fields = await ctx.get(\"total_fields\")\n",
        "\n",
        "        responses = ctx.collect_events(ev, [ResponseEvent] * total_fields)\n",
        "        if responses is None:\n",
        "            return None # do nothing if there's nothing to do yet\n",
        "        print(f\"Responses: {responses}\")\n",
        "        # we've got all the responses!\n",
        "        responseList = \"\\n\".join(\"Field: \" + r.field + \"\\n\" + \"Response: \" + r.response for r in responses)\n",
        "        print(f\"Response list: {responseList}\")\n",
        "        result = self.llm.complete(f\"\"\"\n",
        "            You are given a list of fields in an application form and responses to\n",
        "            questions about those fields from a resume. Combine the two into a list of\n",
        "            fields and fill in the details for me.\n",
        "\n",
        "            <responses>\n",
        "            {responseList}\n",
        "            </responses>\n",
        "        \"\"\")\n",
        "        return StopEvent(result=result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uaAOzkwNJZJh",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uaAOzkwNJZJh",
        "outputId": "d56b7fe3-ec46-425c-d271-4972978b28a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading llama_index.core.storage.kvstore.simple_kvstore from /content/storage/docstore.json.\n",
            "Loading llama_index.core.storage.kvstore.simple_kvstore from /content/storage/index_store.json.\n",
            "Parsing the form...\n",
            "WARNING: content_guideline_instruction is deprecated and may be remove in a future release. Use system_prompt, system_prompt_append or user_prompt instead.\n",
            "WARNING: formatting_instruction is deprecated and may be remove in a future release. Use system_prompt, system_prompt_append or user_prompt instead.\n",
            "Started parsing the file under job_id 29c87be8-c0fa-4027-b26d-f84fdb59f165\n",
            "Fields: ['First Name', 'Last Name', 'Email', 'Phone', 'Linkedin', 'Project Portfolio', 'Degree', 'Graduation Date', 'Current Job Title', 'Current Employer', 'Technical Skills', 'Describe why you’re a good fit for this position', 'Do you have 5 years of experience in React?']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-54-3324100568.py:79: DeprecationWarning: Context.set(key, value) is deprecated. Use 'await ctx.store.set(key, value)' instead.\n",
            "  await ctx.set(\"total_fields\", len(fields))\n",
            "/tmp/ipython-input-54-3324100568.py:91: DeprecationWarning: Context.get() is deprecated. Use 'await ctx.store.get()' instead.\n",
            "  total_fields = await ctx.get(\"total_fields\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Responses: [ResponseEvent(response='Sarah'), ResponseEvent(response='Chen'), ResponseEvent(response='sarah.chen@email.com'), ResponseEvent(response=\"The candidate's phone number is not provided in the information available.\"), ResponseEvent(response=\"The candidate's LinkedIn profile can be found at linkedin.com/in/sarahchen.\"), ResponseEvent(response='The candidate has a project portfolio that includes notable projects such as EcoTrack and ChatFlow. EcoTrack is a full-stack application designed for tracking carbon footprints, utilizing React, Node.js, and MongoDB, and features a machine learning algorithm for personalized sustainability recommendations. It was recognized in TechCrunch\\'s \"Top 10 Environmental Impact Apps of 2023.\" ChatFlow is a real-time chat application developed with the WebSocket protocol and React, featuring end-to-end encryption and message persistence, serving over 5000 monthly active users. These projects highlight the candidate\\'s skills in full-stack development and their ability to create impactful applications.'), ResponseEvent(response='The candidate holds a Bachelor of Science in Computer Science from the University of California, Berkeley.'), ResponseEvent(response='The candidate graduated in 2017.'), ResponseEvent(response='Senior Full Stack Developer'), ResponseEvent(response='The current employer of the candidate is TechFlow Solutions, located in San Francisco, CA.'), ResponseEvent(response='The candidate possesses a robust set of technical skills across both frontend and backend development. On the frontend, they are proficient in React.js, Redux, Next.js, TypeScript, Vue.js, and Nuxt.js, along with HTML5, CSS3, and SASS/SCSS. They also have experience with testing frameworks like Jest and React Testing Library, as well as build tools such as WebPack and Babel.\\n\\nOn the backend, the candidate is skilled in Node.js and Express.js, as well as Python and Django. They have experience with both GraphQL and REST APIs, and are familiar with databases like PostgreSQL and MongoDB. This diverse skill set indicates a strong capability in full stack development, enabling them to handle various aspects of web application development effectively.'), ResponseEvent(response='I am a strong fit for this position due to my extensive experience as a Full Stack Web Developer, with over 6 years of hands-on expertise in crafting scalable web applications and microservices. My proficiency in technologies such as React and Node.js aligns well with the requirements of the role. \\n\\nIn my current position, I successfully architected a microservices-based e-commerce platform that serves over 100,000 daily users, demonstrating my ability to handle high-traffic applications. Additionally, I have led a team of developers in rebuilding a flagship product, showcasing my leadership skills and commitment to quality through established coding standards that improved code quality significantly.\\n\\nMy technical skills extend to both frontend and backend development, including experience with GraphQL, which has allowed me to optimize API performance effectively. I am also passionate about clean code and accessibility, having collaborated with UX teams to implement features that comply with WCAG standards.\\n\\nFurthermore, my certifications in cloud architecture and my involvement in mentoring junior developers reflect my dedication to continuous learning and team development. I am excited about the opportunity to contribute my skills and experience to your team.'), ResponseEvent(response='The candidate has over 6 years of experience as a Full Stack Web Developer, with specific expertise in React, among other technologies.')]\n",
            "Response list: Field: First Name\n",
            "Response: Sarah\n",
            "Field: Last Name\n",
            "Response: Chen\n",
            "Field: Email\n",
            "Response: sarah.chen@email.com\n",
            "Field: Phone\n",
            "Response: The candidate's phone number is not provided in the information available.\n",
            "Field: Linkedin\n",
            "Response: The candidate's LinkedIn profile can be found at linkedin.com/in/sarahchen.\n",
            "Field: Project Portfolio\n",
            "Response: The candidate has a project portfolio that includes notable projects such as EcoTrack and ChatFlow. EcoTrack is a full-stack application designed for tracking carbon footprints, utilizing React, Node.js, and MongoDB, and features a machine learning algorithm for personalized sustainability recommendations. It was recognized in TechCrunch's \"Top 10 Environmental Impact Apps of 2023.\" ChatFlow is a real-time chat application developed with the WebSocket protocol and React, featuring end-to-end encryption and message persistence, serving over 5000 monthly active users. These projects highlight the candidate's skills in full-stack development and their ability to create impactful applications.\n",
            "Field: Degree\n",
            "Response: The candidate holds a Bachelor of Science in Computer Science from the University of California, Berkeley.\n",
            "Field: Graduation Date\n",
            "Response: The candidate graduated in 2017.\n",
            "Field: Current Job Title\n",
            "Response: Senior Full Stack Developer\n",
            "Field: Current Employer\n",
            "Response: The current employer of the candidate is TechFlow Solutions, located in San Francisco, CA.\n",
            "Field: Technical Skills\n",
            "Response: The candidate possesses a robust set of technical skills across both frontend and backend development. On the frontend, they are proficient in React.js, Redux, Next.js, TypeScript, Vue.js, and Nuxt.js, along with HTML5, CSS3, and SASS/SCSS. They also have experience with testing frameworks like Jest and React Testing Library, as well as build tools such as WebPack and Babel.\n",
            "\n",
            "On the backend, the candidate is skilled in Node.js and Express.js, as well as Python and Django. They have experience with both GraphQL and REST APIs, and are familiar with databases like PostgreSQL and MongoDB. This diverse skill set indicates a strong capability in full stack development, enabling them to handle various aspects of web application development effectively.\n",
            "Field: Describe why you’re a good fit for this position\n",
            "Response: I am a strong fit for this position due to my extensive experience as a Full Stack Web Developer, with over 6 years of hands-on expertise in crafting scalable web applications and microservices. My proficiency in technologies such as React and Node.js aligns well with the requirements of the role. \n",
            "\n",
            "In my current position, I successfully architected a microservices-based e-commerce platform that serves over 100,000 daily users, demonstrating my ability to handle high-traffic applications. Additionally, I have led a team of developers in rebuilding a flagship product, showcasing my leadership skills and commitment to quality through established coding standards that improved code quality significantly.\n",
            "\n",
            "My technical skills extend to both frontend and backend development, including experience with GraphQL, which has allowed me to optimize API performance effectively. I am also passionate about clean code and accessibility, having collaborated with UX teams to implement features that comply with WCAG standards.\n",
            "\n",
            "Furthermore, my certifications in cloud architecture and my involvement in mentoring junior developers reflect my dedication to continuous learning and team development. I am excited about the opportunity to contribute my skills and experience to your team.\n",
            "Field: Do you have 5 years of experience in React?\n",
            "Response: The candidate has over 6 years of experience as a Full Stack Web Developer, with specific expertise in React, among other technologies.\n",
            "Here is the combined list of fields and details based on the provided responses:\n",
            "\n",
            "1. **First Name**: Sarah\n",
            "2. **Last Name**: Chen\n",
            "3. **Email**: sarah.chen@email.com\n",
            "4. **Phone**: The candidate's phone number is not provided in the information available.\n",
            "5. **LinkedIn**: [linkedin.com/in/sarahchen](https://linkedin.com/in/sarahchen)\n",
            "6. **Project Portfolio**: \n",
            "   - **EcoTrack**: A full-stack application designed for tracking carbon footprints, utilizing React, Node.js, and MongoDB, featuring a machine learning algorithm for personalized sustainability recommendations. Recognized in TechCrunch's \"Top 10 Environmental Impact Apps of 2023.\"\n",
            "   - **ChatFlow**: A real-time chat application developed with the WebSocket protocol and React, featuring end-to-end encryption and message persistence, serving over 5000 monthly active users. \n",
            "7. **Degree**: Bachelor of Science in Computer Science\n",
            "8. **Graduation Date**: 2017\n",
            "9. **Current Job Title**: Senior Full Stack Developer\n",
            "10. **Current Employer**: TechFlow Solutions, San Francisco, CA\n",
            "11. **Technical Skills**: \n",
            "    - **Frontend**: Proficient in React.js, Redux, Next.js, TypeScript, Vue.js, Nuxt.js, HTML5, CSS3, SASS/SCSS, testing frameworks (Jest, React Testing Library), build tools (WebPack, Babel).\n",
            "    - **Backend**: Skilled in Node.js, Express.js, Python, Django, GraphQL, REST APIs, databases (PostgreSQL, MongoDB).\n",
            "12. **Describe why you’re a good fit for this position**: \n",
            "    - Over 6 years of experience as a Full Stack Web Developer with expertise in crafting scalable web applications and microservices.\n",
            "    - Successfully architected a microservices-based e-commerce platform serving over 100,000 daily users.\n",
            "    - Led a team in rebuilding a flagship product, improving code quality through established coding standards.\n",
            "    - Proficient in both frontend and backend development, with experience in GraphQL for optimizing API performance.\n",
            "    - Passionate about clean code and accessibility, collaborating with UX teams to implement WCAG-compliant features.\n",
            "    - Certifications in cloud architecture and mentoring junior developers reflect dedication to continuous learning and team development.\n",
            "13. **Do you have 5 years of experience in React?**: The candidate has over 6 years of experience as a Full Stack Web Developer, with specific expertise in React, among other technologies.\n"
          ]
        }
      ],
      "source": [
        "w = RAGWorkflow(timeout=120, verbose=False)\n",
        "result = await w.run(\n",
        "    resume_file=\"/content/fake_resume.pdf\",\n",
        "    application_form=\"/content/fake_application_form.pdf\"\n",
        ")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lZkLUb-ENTAu",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 842,
          "resources": {
            "http://localhost:8080/lib/bindings/utils.js": {
              "data": "CjwhRE9DVFlQRSBodG1sPgo8aHRtbCBsYW5nPWVuPgogIDxtZXRhIGNoYXJzZXQ9dXRmLTg+CiAgPG1ldGEgbmFtZT12aWV3cG9ydCBjb250ZW50PSJpbml0aWFsLXNjYWxlPTEsIG1pbmltdW0tc2NhbGU9MSwgd2lkdGg9ZGV2aWNlLXdpZHRoIj4KICA8dGl0bGU+RXJyb3IgNDA0IChOb3QgRm91bmQpISExPC90aXRsZT4KICA8c3R5bGU+CiAgICAqe21hcmdpbjowO3BhZGRpbmc6MH1odG1sLGNvZGV7Zm9udDoxNXB4LzIycHggYXJpYWwsc2Fucy1zZXJpZn1odG1se2JhY2tncm91bmQ6I2ZmZjtjb2xvcjojMjIyO3BhZGRpbmc6MTVweH1ib2R5e21hcmdpbjo3JSBhdXRvIDA7bWF4LXdpZHRoOjM5MHB4O21pbi1oZWlnaHQ6MTgwcHg7cGFkZGluZzozMHB4IDAgMTVweH0qID4gYm9keXtiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9lcnJvcnMvcm9ib3QucG5nKSAxMDAlIDVweCBuby1yZXBlYXQ7cGFkZGluZy1yaWdodDoyMDVweH1we21hcmdpbjoxMXB4IDAgMjJweDtvdmVyZmxvdzpoaWRkZW59aW5ze2NvbG9yOiM3Nzc7dGV4dC1kZWNvcmF0aW9uOm5vbmV9YSBpbWd7Ym9yZGVyOjB9QG1lZGlhIHNjcmVlbiBhbmQgKG1heC13aWR0aDo3NzJweCl7Ym9keXtiYWNrZ3JvdW5kOm5vbmU7bWFyZ2luLXRvcDowO21heC13aWR0aDpub25lO3BhZGRpbmctcmlnaHQ6MH19I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LnBuZykgbm8tcmVwZWF0O21hcmdpbi1sZWZ0Oi01cHh9QG1lZGlhIG9ubHkgc2NyZWVuIGFuZCAobWluLXJlc29sdXRpb246MTkyZHBpKXsjbG9nb3tiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSBuby1yZXBlYXQgMCUgMCUvMTAwJSAxMDAlOy1tb3otYm9yZGVyLWltYWdlOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSAwfX1AbWVkaWEgb25seSBzY3JlZW4gYW5kICgtd2Via2l0LW1pbi1kZXZpY2UtcGl4ZWwtcmF0aW86Mil7I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LTJ4LnBuZykgbm8tcmVwZWF0Oy13ZWJraXQtYmFja2dyb3VuZC1zaXplOjEwMCUgMTAwJX19I2xvZ297ZGlzcGxheTppbmxpbmUtYmxvY2s7aGVpZ2h0OjU0cHg7d2lkdGg6MTUwcHh9CiAgPC9zdHlsZT4KICA8YSBocmVmPS8vd3d3Lmdvb2dsZS5jb20vPjxzcGFuIGlkPWxvZ28gYXJpYS1sYWJlbD1Hb29nbGU+PC9zcGFuPjwvYT4KICA8cD48Yj40MDQuPC9iPiA8aW5zPlRoYXTigJlzIGFuIGVycm9yLjwvaW5zPgogIDxwPiAgPGlucz5UaGF04oCZcyBhbGwgd2Uga25vdy48L2lucz4K",
              "headers": [
                [
                  "content-length",
                  "1449"
                ],
                [
                  "content-type",
                  "text/html; charset=utf-8"
                ]
              ],
              "ok": false,
              "status": 404,
              "status_text": ""
            }
          }
        },
        "id": "lZkLUb-ENTAu",
        "outputId": "9ce89473-5053-4594-9f5a-38539f7eb1c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/workflows/FormParser_workflow.html\n"
          ]
        },
        {
          "data": {
            "text/html": [
              " <div style=\"width: 100%; height: 800px; overflow: hidden;\"> <html>\n",
              "    <head>\n",
              "        <meta charset=\"utf-8\">\n",
              "        \n",
              "            <script src=\"lib/bindings/utils.js\"></script>\n",
              "            <link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/dist/vis-network.min.css\" integrity=\"sha512-WgxfT5LWjfszlPHXRmBWHkV2eceiWTOBvrKCNbdgDYTHrT2AeLCGbF4sZlZw3UMN3WtL0tGUoIAKsu8mllg/XA==\" crossorigin=\"anonymous\" referrerpolicy=\"no-referrer\" />\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/vis-network.min.js\" integrity=\"sha512-LnvoEWDFrqGHlHmDD2101OrLcbsfkrzoSpvtSQtxK3RMnRV0eOkhhBN2dXHKRrUU8p2DGRTk35n4O8nWSVe1mQ==\" crossorigin=\"anonymous\" referrerpolicy=\"no-referrer\"></script>\n",
              "            \n",
              "        \n",
              "<center>\n",
              "<h1></h1>\n",
              "</center>\n",
              "\n",
              "<!-- <link rel=\"stylesheet\" href=\"../node_modules/vis/dist/vis.min.css\" type=\"text/css\" />\n",
              "<script type=\"text/javascript\" src=\"../node_modules/vis/dist/vis.js\"> </script>-->\n",
              "        <link\n",
              "          href=\"https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/css/bootstrap.min.css\"\n",
              "          rel=\"stylesheet\"\n",
              "          integrity=\"sha384-eOJMYsd53ii+scO/bJGFsiCZc+5NDVN2yr8+0RDqr0Ql0h+rP48ckxlpbzKgwra6\"\n",
              "          crossorigin=\"anonymous\"\n",
              "        />\n",
              "        <script\n",
              "          src=\"https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/js/bootstrap.bundle.min.js\"\n",
              "          integrity=\"sha384-JEW9xMcG8R+pH31jmWH6WWP0WintQrMb4s7ZOdauHnUtxwoG2vI5DkLtS3qm9Ekf\"\n",
              "          crossorigin=\"anonymous\"\n",
              "        ></script>\n",
              "\n",
              "\n",
              "        <center>\n",
              "          <h1></h1>\n",
              "        </center>\n",
              "        <style type=\"text/css\">\n",
              "\n",
              "             #mynetwork {\n",
              "                 width: 100%;\n",
              "                 height: 750px;\n",
              "                 background-color: #ffffff;\n",
              "                 border: 1px solid lightgray;\n",
              "                 position: relative;\n",
              "                 float: left;\n",
              "             }\n",
              "\n",
              "             \n",
              "\n",
              "             \n",
              "\n",
              "             \n",
              "        </style>\n",
              "    </head>\n",
              "\n",
              "\n",
              "    <body>\n",
              "        <div class=\"card\" style=\"width: 100%\">\n",
              "            \n",
              "            \n",
              "            <div id=\"mynetwork\" class=\"card-body\"></div>\n",
              "        </div>\n",
              "\n",
              "        \n",
              "        \n",
              "\n",
              "        <script type=\"text/javascript\">\n",
              "\n",
              "              // initialize global variables.\n",
              "              var edges;\n",
              "              var nodes;\n",
              "              var allNodes;\n",
              "              var allEdges;\n",
              "              var nodeColors;\n",
              "              var originalNodes;\n",
              "              var network;\n",
              "              var container;\n",
              "              var options, data;\n",
              "              var filter = {\n",
              "                  item : '',\n",
              "                  property : '',\n",
              "                  value : []\n",
              "              };\n",
              "\n",
              "              \n",
              "\n",
              "              \n",
              "\n",
              "              // This method is responsible for drawing the graph, returns the drawn network\n",
              "              function drawGraph() {\n",
              "                  var container = document.getElementById('mynetwork');\n",
              "\n",
              "                  \n",
              "\n",
              "                  // parsing and collecting nodes and edges from the python\n",
              "                  nodes = new vis.DataSet([{\"color\": \"#ADD8E6\", \"id\": \"_done\", \"label\": \"_done\", \"shape\": \"box\", \"title\": null}, {\"color\": \"#FFA07A\", \"id\": \"StopEvent\", \"label\": \"StopEvent\", \"shape\": \"ellipse\", \"title\": null}, {\"color\": \"#ADD8E6\", \"id\": \"ask_question\", \"label\": \"ask_question\", \"shape\": \"box\", \"title\": null}, {\"color\": \"#90EE90\", \"id\": \"QueryEvent\", \"label\": \"QueryEvent\", \"shape\": \"ellipse\", \"title\": null}, {\"color\": \"#90EE90\", \"id\": \"ResponseEvent\", \"label\": \"ResponseEvent\", \"shape\": \"ellipse\", \"title\": null}, {\"color\": \"#ADD8E6\", \"id\": \"fill_in_application\", \"label\": \"fill_in_application\", \"shape\": \"box\", \"title\": null}, {\"color\": \"#ADD8E6\", \"id\": \"parse_form\", \"label\": \"parse_form\", \"shape\": \"box\", \"title\": null}, {\"color\": \"#90EE90\", \"id\": \"ParseFormEvent\", \"label\": \"ParseFormEvent\", \"shape\": \"ellipse\", \"title\": null}, {\"color\": \"#ADD8E6\", \"id\": \"set_up\", \"label\": \"set_up\", \"shape\": \"box\", \"title\": null}, {\"color\": \"#E27AFF\", \"id\": \"StartEvent\", \"label\": \"StartEvent\", \"shape\": \"ellipse\", \"title\": null}]);\n",
              "                  edges = new vis.DataSet([{\"arrows\": \"to\", \"from\": \"StopEvent\", \"to\": \"_done\"}, {\"arrows\": \"to\", \"from\": \"ask_question\", \"to\": \"ResponseEvent\"}, {\"arrows\": \"to\", \"from\": \"QueryEvent\", \"to\": \"ask_question\"}, {\"arrows\": \"to\", \"from\": \"fill_in_application\", \"to\": \"StopEvent\"}, {\"arrows\": \"to\", \"from\": \"ResponseEvent\", \"to\": \"fill_in_application\"}, {\"arrows\": \"to\", \"from\": \"parse_form\", \"to\": \"QueryEvent\"}, {\"arrows\": \"to\", \"from\": \"ParseFormEvent\", \"to\": \"parse_form\"}, {\"arrows\": \"to\", \"from\": \"set_up\", \"to\": \"ParseFormEvent\"}, {\"arrows\": \"to\", \"from\": \"StartEvent\", \"to\": \"set_up\"}]);\n",
              "\n",
              "                  nodeColors = {};\n",
              "                  allNodes = nodes.get({ returnType: \"Object\" });\n",
              "                  for (nodeId in allNodes) {\n",
              "                    nodeColors[nodeId] = allNodes[nodeId].color;\n",
              "                  }\n",
              "                  allEdges = edges.get({ returnType: \"Object\" });\n",
              "                  // adding nodes and edges to the graph\n",
              "                  data = {nodes: nodes, edges: edges};\n",
              "\n",
              "                  var options = {\n",
              "    \"configure\": {\n",
              "        \"enabled\": false\n",
              "    },\n",
              "    \"edges\": {\n",
              "        \"color\": {\n",
              "            \"inherit\": true\n",
              "        },\n",
              "        \"smooth\": {\n",
              "            \"enabled\": true,\n",
              "            \"type\": \"dynamic\"\n",
              "        }\n",
              "    },\n",
              "    \"interaction\": {\n",
              "        \"dragNodes\": true,\n",
              "        \"hideEdgesOnDrag\": false,\n",
              "        \"hideNodesOnDrag\": false\n",
              "    },\n",
              "    \"physics\": {\n",
              "        \"enabled\": true,\n",
              "        \"stabilization\": {\n",
              "            \"enabled\": true,\n",
              "            \"fit\": true,\n",
              "            \"iterations\": 1000,\n",
              "            \"onlyDynamicEdges\": false,\n",
              "            \"updateInterval\": 50\n",
              "        }\n",
              "    }\n",
              "};\n",
              "\n",
              "                  \n",
              "\n",
              "\n",
              "                  \n",
              "\n",
              "                  network = new vis.Network(container, data, options);\n",
              "\n",
              "                  \n",
              "\n",
              "                  \n",
              "\n",
              "                  \n",
              "\n",
              "\n",
              "                  \n",
              "\n",
              "                  return network;\n",
              "\n",
              "              }\n",
              "              drawGraph();\n",
              "        </script>\n",
              "    </body>\n",
              "</html> </div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "isolated": true
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "WORKFLOW_FILE = \"/content/workflows/FormParser_workflow.html\"\n",
        "draw_all_possible_flows(w, filename=WORKFLOW_FILE)\n",
        "html_content = extract_html_content(WORKFLOW_FILE)\n",
        "# display(HTML(html_content), metadata=dict(isolated=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wKfh_tqXMOv-",
      "metadata": {
        "id": "wKfh_tqXMOv-"
      },
      "source": [
        "# **Human In The Loop** (Event Driven)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "NO2QHlxOmuj5",
      "metadata": {
        "id": "NO2QHlxOmuj5"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.workflow import InputRequiredEvent, HumanResponseEvent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "id": "gFPtHY96Ns2w",
      "metadata": {
        "id": "gFPtHY96Ns2w"
      },
      "outputs": [],
      "source": [
        "class ParseFormEvent(Event):\n",
        "    application_form: str\n",
        "\n",
        "class QueryEvent(Event):\n",
        "    query: str\n",
        "    field: str\n",
        "\n",
        "class ResponseEvent(Event):\n",
        "    response: str\n",
        "\n",
        "# class HumanResponseEvent(Event):\n",
        "#     response: str\n",
        "\n",
        "# class InputRequiredEvent(Event):\n",
        "#     prefix: str\n",
        "#     result: str\n",
        "\n",
        "class FeedbackEvent(Event):\n",
        "    feedback: str\n",
        "\n",
        "class GenerateQuestionsEvent(Event):\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "-w0gazWzN6Kd",
      "metadata": {
        "id": "-w0gazWzN6Kd"
      },
      "outputs": [],
      "source": [
        "class RAGWorkflow(Workflow):\n",
        "\n",
        "    storage_dir = \"/content/storage\"\n",
        "    llm: OpenAI\n",
        "    query_engine: VectorStoreIndex\n",
        "\n",
        "    @step\n",
        "    async def set_up(self, ctx: Context, ev: StartEvent) -> ParseFormEvent:\n",
        "\n",
        "        if not ev.resume_file:\n",
        "            raise ValueError(\"No resume file provided\")\n",
        "\n",
        "        if not ev.application_form:\n",
        "            raise ValueError(\"No application form provided\")\n",
        "\n",
        "        # define the LLM to work with\n",
        "        self.llm = OpenAI(model=\"gpt-4o-mini\")\n",
        "\n",
        "        # ingest the data and set up the query engine\n",
        "        if os.path.exists(self.storage_dir):\n",
        "            # you've already ingested the resume document\n",
        "            storage_context = StorageContext.from_defaults(persist_dir=\n",
        "                                                           self.storage_dir)\n",
        "            index = load_index_from_storage(storage_context)\n",
        "        else:\n",
        "            # parse and load the resume document\n",
        "            documents = LlamaParse(\n",
        "                api_key=os.getenv(\"LLAMA_CLOUD_API_KEY\"),\n",
        "                base_url=os.getenv(\"LLAMA_CLOUD_BASE_URL\"),\n",
        "                result_type=\"markdown\",\n",
        "                content_guideline_instruction=\"This is a resume, gather related facts together and format it as bullet points with headers\"\n",
        "            ).load_data(ev.resume_file)\n",
        "            # embed and index the documents\n",
        "            index = VectorStoreIndex.from_documents(\n",
        "                documents,\n",
        "                embed_model=OpenAIEmbedding(model_name=\"text-embedding-3-small\")\n",
        "            )\n",
        "            index.storage_context.persist(persist_dir=self.storage_dir)\n",
        "\n",
        "        # create a query engine\n",
        "        self.query_engine = index.as_query_engine(llm=self.llm, similarity_top_k=5)\n",
        "\n",
        "        # let's pass the application form to a new step to parse it\n",
        "        return ParseFormEvent(application_form=ev.application_form)\n",
        "\n",
        "    # form parsing\n",
        "    @step\n",
        "    async def parse_form(self, ctx: Context, ev: ParseFormEvent) -> GenerateQuestionsEvent:\n",
        "        parser = LlamaParse(\n",
        "            api_key=os.getenv(\"LLAMA_CLOUD_API_KEY\"),\n",
        "            base_url=os.getenv(\"LLAMA_CLOUD_BASE_URL\"),\n",
        "            result_type=\"markdown\",\n",
        "            content_guideline_instruction=\"This is a job application form. Create a list of all the fields that need to be filled in.\",\n",
        "            formatting_instruction=\"Return a bulleted list of the fields ONLY.\"\n",
        "        )\n",
        "\n",
        "        # get the LLM to convert the parsed form into JSON\n",
        "        result = parser.load_data(ev.application_form)[0]\n",
        "        raw_json = self.llm.complete(\n",
        "            f\"This is a parsed form. Convert it into a JSON object containing only the list of fields to be filled in, in the form {{ fields: [...] }}. <form>{result.text}</form>. Return JSON ONLY, no markdown.\")\n",
        "        fields = json.loads(raw_json.text)[\"fields\"]\n",
        "\n",
        "        await ctx.set(\"fields_to_fill\", fields)\n",
        "\n",
        "        return GenerateQuestionsEvent()\n",
        "\n",
        "    # generate questions\n",
        "    @step\n",
        "    async def generate_questions(self, ctx: Context, ev: GenerateQuestionsEvent | FeedbackEvent) -> QueryEvent:\n",
        "\n",
        "        # get the list of fields to fill in\n",
        "        fields = await ctx.get(\"fields_to_fill\")\n",
        "\n",
        "        # generate one query for each of the fields, and fire them off\n",
        "        for field in fields:\n",
        "            question = f\"How would you answer this question about the candidate? <field>{field}</field>\"\n",
        "\n",
        "            # new! Is there feedback? If so, add it to the query:\n",
        "            if hasattr(ev,\"feedback\"):\n",
        "                question += f\"\"\"\n",
        "                    \\nWe previously got feedback about how we answered the questions.\n",
        "                    It might not be relevant to this particular field, but here it is:\n",
        "                    <feedback>{ev.feedback}</feedback>\n",
        "                \"\"\"\n",
        "\n",
        "            ctx.send_event(QueryEvent(\n",
        "                field=field,\n",
        "                query=question\n",
        "            ))\n",
        "\n",
        "        # store the number of fields so we know how many to wait for later\n",
        "        await ctx.set(\"total_fields\", len(fields))\n",
        "        return\n",
        "\n",
        "    @step\n",
        "    async def ask_question(self, ctx: Context, ev: QueryEvent) -> ResponseEvent:\n",
        "        response = self.query_engine.query(f\"This is a question about the specific resume we have in our database: {ev.query}\")\n",
        "        return ResponseEvent(field=ev.field, response=response.response)\n",
        "\n",
        "\n",
        "    # Get feedback from the human\n",
        "    @step\n",
        "    async def fill_in_application(self, ctx: Context, ev: ResponseEvent) -> InputRequiredEvent:\n",
        "        # get the total number of fields to wait for\n",
        "        total_fields = await ctx.get(\"total_fields\")\n",
        "\n",
        "        responses = ctx.collect_events(ev, [ResponseEvent] * total_fields)\n",
        "        if responses is None:\n",
        "            return None # do nothing if there's nothing to do yet\n",
        "\n",
        "        # we've got all the responses!\n",
        "        responseList = \"\\n\".join(\"Field: \" + r.field + \"\\n\" + \"Response: \" + r.response for r in responses)\n",
        "\n",
        "        result = self.llm.complete(f\"\"\"\n",
        "            You are given a list of fields in an application form and responses to\n",
        "            questions about those fields from a resume. Combine the two into a list of\n",
        "            fields and succinct, factual answers to fill in those fields.\n",
        "\n",
        "            <responses>\n",
        "            {responseList}\n",
        "            </responses>\n",
        "        \"\"\")\n",
        "\n",
        "        # save the result for later\n",
        "        await ctx.set(\"filled_form\", str(result))\n",
        "\n",
        "        # Fire off the feedback request\n",
        "        return InputRequiredEvent(\n",
        "            prefix=\"How does this look? Give me any feedback you have on any of the answers.\",\n",
        "            result=result\n",
        "        )\n",
        "\n",
        "    # Accept the feedback when a HumanResponseEvent fires\n",
        "    @step\n",
        "    async def get_feedback(self, ctx: Context, ev: HumanResponseEvent) -> FeedbackEvent | StopEvent:\n",
        "\n",
        "        result = self.llm.complete(f\"\"\"\n",
        "            You have received some human feedback on the form-filling task you've done.\n",
        "            Does everything look good, or is there more work to be done?\n",
        "            <feedback>\n",
        "            {ev.response}\n",
        "            </feedback>\n",
        "            If everything is fine, respond with just the word 'OKAY'.\n",
        "            If there's any other feedback, respond with just the word 'FEEDBACK'.\n",
        "        \"\"\")\n",
        "\n",
        "        verdict = result.text.strip()\n",
        "\n",
        "        print(f\"LLM says the verdict was {verdict}\")\n",
        "        if (verdict == \"OKAY\"):\n",
        "            return StopEvent(result=await ctx.get(\"filled_form\"))\n",
        "        else:\n",
        "            return FeedbackEvent(feedback=ev.response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "L6niZGZ_QrUM",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6niZGZ_QrUM",
        "outputId": "5df24650-d345-46d7-a1f5-a638b072d169"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading llama_index.core.storage.kvstore.simple_kvstore from /content/storage/docstore.json.\n",
            "Loading llama_index.core.storage.kvstore.simple_kvstore from /content/storage/index_store.json.\n",
            "WARNING: content_guideline_instruction is deprecated and may be remove in a future release. Use system_prompt, system_prompt_append or user_prompt instead.\n",
            "WARNING: formatting_instruction is deprecated and may be remove in a future release. Use system_prompt, system_prompt_append or user_prompt instead.\n",
            "Started parsing the file under job_id ffb7efb7-00ed-4b53-88a2-709e77e45dfd\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-65-3805744654.py:63: DeprecationWarning: Context.set(key, value) is deprecated. Use 'await ctx.store.set(key, value)' instead.\n",
            "  await ctx.set(\"fields_to_fill\", fields)\n",
            "/tmp/ipython-input-65-3805744654.py:72: DeprecationWarning: Context.get() is deprecated. Use 'await ctx.store.get()' instead.\n",
            "  fields = await ctx.get(\"fields_to_fill\")\n",
            "/tmp/ipython-input-65-3805744654.py:92: DeprecationWarning: Context.set(key, value) is deprecated. Use 'await ctx.store.set(key, value)' instead.\n",
            "  await ctx.set(\"total_fields\", len(fields))\n",
            "/tmp/ipython-input-65-3805744654.py:105: DeprecationWarning: Context.get() is deprecated. Use 'await ctx.store.get()' instead.\n",
            "  total_fields = await ctx.get(\"total_fields\")\n",
            "/tmp/ipython-input-65-3805744654.py:125: DeprecationWarning: Context.set(key, value) is deprecated. Use 'await ctx.store.set(key, value)' instead.\n",
            "  await ctx.set(\"filled_form\", str(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "We've filled in your form! Here are the results:\n",
            "\n",
            "Here is the combined list of fields and succinct, factual answers:\n",
            "\n",
            "1. **First Name**: Sarah\n",
            "2. **Last Name**: Chen\n",
            "3. **Email**: sarah.chen@email.com\n",
            "4. **Phone**: Not provided\n",
            "5. **LinkedIn**: linkedin.com/in/sarahchen\n",
            "6. **Project Portfolio**: Notable projects include EcoTrack, a full-stack application for tracking carbon footprints recognized in TechCrunch's \"Top 10 Environmental Impact Apps of 2023,\" and ChatFlow, a real-time chat application with end-to-end encryption serving over 5000 monthly active users.\n",
            "7. **Degree**: Bachelor of Science in Computer Science\n",
            "8. **Graduation Date**: 2017\n",
            "9. **Current Job Title**: Senior Full Stack Developer\n",
            "10. **Current Employer**: TechFlow Solutions\n",
            "11. **Technical Skills**: Proficient in frontend technologies (React.js, Redux, Next.js, TypeScript, Vue.js, HTML5, CSS3, SASS/SCSS) and backend technologies (Node.js, Express.js, Python, Django). Experienced with GraphQL, REST APIs, PostgreSQL, and MongoDB.\n",
            "12. **Describe why you’re a good fit for this position**: Over 6 years of experience in full stack development, expertise in React and Node.js, proven track record in leading teams, implementing CI/CD pipelines, and architecting complex projects. Strong commitment to clean code, accessibility, and mentoring.\n",
            "13. **Do you have 5 years of experience in React?**: Yes, over 6 years of experience.\n",
            "How does this look? Give me any feedback you have on any of the answers.provide me portfolio URL in the form\n",
            "LLM says the verdict was FEEDBACK\n",
            "We've filled in your form! Here are the results:\n",
            "\n",
            "Here is the combined list of fields and succinct, factual answers based on the provided responses:\n",
            "\n",
            "1. **First Name:** Sarah\n",
            "2. **Last Name:** Chen\n",
            "3. **Email:** sarah.chen@email.com\n",
            "4. **Phone:** Not provided\n",
            "5. **LinkedIn:** linkedin.com/in/sarahchen\n",
            "6. **Project Portfolio:** [sarahchen.dev](http://sarahchen.dev)\n",
            "7. **Degree:** Bachelor of Science in Computer Science from the University of California, Berkeley\n",
            "8. **Graduation Date:** May 2017\n",
            "9. **Current Job Title:** Senior Full Stack Developer\n",
            "10. **Current Employer:** TechFlow Solutions\n",
            "11. **Technical Skills:**\n",
            "    - **Frontend:** React.js, Redux, Next.js, TypeScript, Vue.js, Nuxt.js, HTML5, CSS3, SASS/SCSS, Jest, React Testing Library, WebPack, Babel\n",
            "    - **Backend:** Node.js, Express.js, Python, Django, GraphQL, REST APIs, PostgreSQL, MongoDB\n",
            "12. **Why You're a Good Fit:** Over 6 years of experience as a Full Stack Web Developer, leading technical teams and implementing scalable web applications, particularly with React and Node.js. Proven ability to improve code quality and reduce deployment times. Portfolio includes recognized projects like EcoTrack and ChatFlow.\n",
            "13. **Experience in React:** Yes, over 6 years of experience.\n",
            "How does this look? Give me any feedback you have on any of the answers.it is okay\n",
            "LLM says the verdict was OKAY\n",
            "Agent complete! Here's your final result:\n",
            "Here is the combined list of fields and succinct, factual answers based on the provided responses:\n",
            "\n",
            "1. **First Name:** Sarah\n",
            "2. **Last Name:** Chen\n",
            "3. **Email:** sarah.chen@email.com\n",
            "4. **Phone:** Not provided\n",
            "5. **LinkedIn:** linkedin.com/in/sarahchen\n",
            "6. **Project Portfolio:** [sarahchen.dev](http://sarahchen.dev)\n",
            "7. **Degree:** Bachelor of Science in Computer Science from the University of California, Berkeley\n",
            "8. **Graduation Date:** May 2017\n",
            "9. **Current Job Title:** Senior Full Stack Developer\n",
            "10. **Current Employer:** TechFlow Solutions\n",
            "11. **Technical Skills:**\n",
            "    - **Frontend:** React.js, Redux, Next.js, TypeScript, Vue.js, Nuxt.js, HTML5, CSS3, SASS/SCSS, Jest, React Testing Library, WebPack, Babel\n",
            "    - **Backend:** Node.js, Express.js, Python, Django, GraphQL, REST APIs, PostgreSQL, MongoDB\n",
            "12. **Why You're a Good Fit:** Over 6 years of experience as a Full Stack Web Developer, leading technical teams and implementing scalable web applications, particularly with React and Node.js. Proven ability to improve code quality and reduce deployment times. Portfolio includes recognized projects like EcoTrack and ChatFlow.\n",
            "13. **Experience in React:** Yes, over 6 years of experience.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-65-3805744654.py:151: DeprecationWarning: Context.get() is deprecated. Use 'await ctx.store.get()' instead.\n",
            "  return StopEvent(result=await ctx.get(\"filled_form\"))\n"
          ]
        }
      ],
      "source": [
        "w = RAGWorkflow(timeout=600, verbose=False)\n",
        "handler = w.run(\n",
        "    resume_file=\"/content/fake_resume.pdf\",\n",
        "    application_form=\"/content/fake_application_form.pdf\"\n",
        ")\n",
        "\n",
        "async for event in handler.stream_events():\n",
        "    if isinstance(event, InputRequiredEvent):\n",
        "        print(\"We've filled in your form! Here are the results:\\n\")\n",
        "        print(event.result)\n",
        "        # now ask for input from the keyboard\n",
        "        response = input(event.prefix)\n",
        "        handler.ctx.send_event(\n",
        "            HumanResponseEvent(\n",
        "                response=response\n",
        "            )\n",
        "        )\n",
        "\n",
        "response = await handler\n",
        "print(\"Agent complete! Here's your final result:\")\n",
        "print(str(response))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DlJXwezYLsGa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DlJXwezYLsGa",
        "outputId": "5192b64a-d956-425d-ffe4-f44cc3b2ce95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/workflows/FormParser_Human_in_loop_workflow.html\n"
          ]
        }
      ],
      "source": [
        "WORKFLOW_FILE = \"/content/workflows/FormParser_Human_in_loop_workflow.html\"\n",
        "draw_all_possible_flows(w, filename=WORKFLOW_FILE)\n",
        "# html_content = extract_html_content(WORKFLOW_FILE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pUZIxBLKUsnw",
      "metadata": {
        "id": "pUZIxBLKUsnw"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
